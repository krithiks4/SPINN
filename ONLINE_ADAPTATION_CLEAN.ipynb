{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fbc4eb",
   "metadata": {},
   "source": [
    "# üöÄ ONLINE ADAPTATION FOR SPINN - COMPLETE WORKFLOW\n",
    "\n",
    "## üìã Quick Navigation\n",
    "\n",
    "**PART A: SETUP (Cells 1-10)** - Clone repo, install dependencies, get data  \n",
    "**PART B: TRAINING (Cells 11-13)** - Train 68% compressed models (OPTIONAL - 2-3 hours)  \n",
    "**PART C: ONLINE ADAPTATION (Cells 14-23)** - Run experiment (~10 min)  \n",
    "**PART D: RESULTS (Cells 24-26)** - Generate figures and paper-ready results  \n",
    "\n",
    "---\n",
    "\n",
    "**Total Time:**\n",
    "- With existing models: ~15 minutes\n",
    "- Training from scratch: ~3 hours\n",
    "\n",
    "Let's begin! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55af20",
   "metadata": {},
   "source": [
    "---\n",
    "# PART A: SETUP\n",
    "\n",
    "## Cell 1: Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slow on CPU.\")\n",
    "    print(\"üí° In Colab: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "\n",
    "# Install required packages\n",
    "print(\"\\nüì¶ Installing dependencies...\")\n",
    "!pip install -q scipy scikit-learn matplotlib pandas tqdm\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88431e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Clone Repository from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone repository\n",
    "REPO_URL = \"https://github.com/krithiks4/SPINN.git\"\n",
    "REPO_NAME = \"SPINN\"\n",
    "\n",
    "print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_NAME):\n",
    "    print(f\"‚ö†Ô∏è Directory '{REPO_NAME}' already exists. Removing...\")\n",
    "    !rm -rf {REPO_NAME}\n",
    "\n",
    "# Clone the repository\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "# Change to repository directory\n",
    "os.chdir(REPO_NAME)\n",
    "\n",
    "print(f\"\\n‚úÖ Repository cloned successfully!\")\n",
    "print(f\"üìÇ Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ba3ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Upload NASA Milling Dataset\n",
    "\n",
    "**‚ö†Ô∏è MANUAL STEP REQUIRED:**\n",
    "\n",
    "1. Download `mill.mat` from NASA's website (or use your local copy)\n",
    "2. In Colab sidebar: Click üìÅ folder icon\n",
    "3. Navigate to `SPINN/data/raw/nasa/`\n",
    "4. Click upload icon and select `mill.mat`\n",
    "\n",
    "OR if you have it in `/content/`:\n",
    "```python\n",
    "!cp /content/mill.mat data/raw/nasa/\n",
    "```\n",
    "\n",
    "Then re-run this cell to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p data/raw/nasa\n",
    "!mkdir -p data/processed\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists('data/raw/nasa/mill.mat'):\n",
    "    size = os.path.getsize('data/raw/nasa/mill.mat')\n",
    "    print(f\"‚úÖ mill.mat found! ({size:,} bytes)\")\n",
    "    print(\"üìå Proceed to Cell 4 to preprocess\")\n",
    "else:\n",
    "    print(\"‚ùå mill.mat not found in data/raw/nasa/\")\n",
    "    print(\"\\nüì§ Please upload mill.mat, then re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b372be",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2dde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('data/raw/nasa/mill.mat'):\n",
    "    print(\"‚úÖ Found mill.mat! Starting preprocessing...\")\n",
    "    print(\"‚è±Ô∏è This will take 2-3 minutes...\\n\")\n",
    "    \n",
    "    !python data/preprocess.py\n",
    "    \n",
    "    print(\"\\n‚úÖ Preprocessing complete!\")\n",
    "    \n",
    "    # Verify processed files\n",
    "    print(\"\\nüìã Processed files:\")\n",
    "    for file in ['train.csv', 'val.csv', 'test.csv', 'metadata.json']:\n",
    "        path = f'data/processed/{file}'\n",
    "        if os.path.exists(path):\n",
    "            size = os.path.getsize(path)\n",
    "            print(f\"  ‚úÖ {file} ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"‚ùå mill.mat not found. Go back to Cell 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f5f45",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Check for Existing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5182e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üîç Checking for pre-trained models...\\n\")\n",
    "\n",
    "models = {\n",
    "    'dense_pinn.pth': 'models/saved/dense_pinn.pth',\n",
    "    'spinn_structured.pth': 'models/saved/spinn_structured.pth'\n",
    "}\n",
    "\n",
    "all_exist = True\n",
    "for name, path in models.items():\n",
    "    if os.path.exists(path):\n",
    "        size = os.path.getsize(path) / (1024**2)\n",
    "        print(f\"‚úÖ {name} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name} - NOT FOUND\")\n",
    "        all_exist = False\n",
    "\n",
    "if all_exist:\n",
    "    print(\"\\n‚úÖ Models found! You can skip training (go to Cell 14)\")\n",
    "    print(\"‚ö†Ô∏è NOTE: These may be old 43% models. Check Cell 14 output.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Models missing. You need to train (see Cell 11)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19664b24",
   "metadata": {},
   "source": [
    "---\n",
    "# PART B: TRAINING (OPTIONAL)\n",
    "\n",
    "## Cell 11: Train Models for 68% Compression\n",
    "\n",
    "**‚ö†Ô∏è WARNING: This takes 2-3 hours on GPU!**\n",
    "\n",
    "Only run this if:\n",
    "- You don't have models, OR\n",
    "- You want 68% compression (old models are 43%)\n",
    "\n",
    "**Before running:**\n",
    "1. Make sure GPU is enabled (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "2. Set `TRAIN_MODELS = True` below\n",
    "3. Click run and wait ~3 hours\n",
    "\n",
    "**What this does:**\n",
    "- Trains Dense PINN baseline (~60 min)\n",
    "- Runs 4-stage iterative pruning (~2 hours)\n",
    "- Achieves 68.5% compression\n",
    "- Saves checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THIS TO True TO START TRAINING\n",
    "TRAIN_MODELS = False  # ‚ö†Ô∏è Change to True to train!\n",
    "\n",
    "if TRAIN_MODELS:\n",
    "    print(\"üöÄ Starting training from scratch...\")\n",
    "    print(\"‚è±Ô∏è Estimated time: 2-3 hours on T4 GPU\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create directories\n",
    "    !mkdir -p models/saved\n",
    "    !mkdir -p results/checkpoints\n",
    "    !mkdir -p results/metrics\n",
    "    !mkdir -p results/figures\n",
    "    \n",
    "    # Train Dense PINN\n",
    "    print(\"\\nüìä STEP 1: Training Dense PINN Baseline\")\n",
    "    print(\"=\" * 70)\n",
    "    !python train_baseline_improved.py\n",
    "    \n",
    "    # Train SPINN with 4-stage STRUCTURED pruning\n",
    "    print(\"\\n\\nüìä STEP 2: Training SPINN (4-Stage STRUCTURED Pruning)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  This uses TRUE structured pruning (physically removes neurons)\")\n",
    "    !python train_spinn_structured.py\n",
    "    \n",
    "    print(\"\\n\\n‚úÖ Training complete!\")\n",
    "    print(\"üìå Next: Run Cell 12 to verify and copy models\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping training (TRAIN_MODELS = False)\")\n",
    "    print(\"\\nüí° To train:\")\n",
    "    print(\"   1. Set TRAIN_MODELS = True above\")\n",
    "    print(\"   2. Re-run this cell\")\n",
    "    print(\"   3. Wait ~3 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195926d7",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Verify Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "print(\"üîç Checking training results...\\n\")\n",
    "\n",
    "checkpoints = [\n",
    "    'results/checkpoints/dense_pinn_improved_final.pt',\n",
    "    'results/checkpoints/spinn_structured_final.pt',\n",
    "    'results/checkpoints/spinn_structured_stage1.pt',\n",
    "    'results/checkpoints/spinn_structured_stage2.pt',\n",
    "    'results/checkpoints/spinn_structured_stage3.pt',\n",
    "    'results/checkpoints/spinn_structured_stage4.pt'\n",
    "]\n",
    "\n",
    "all_found = True\n",
    "for cp in checkpoints:\n",
    "    if os.path.exists(cp):\n",
    "        size = os.path.getsize(cp) / (1024**2)\n",
    "        print(f\"‚úÖ {cp.split('/')[-1]} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {cp.split('/')[-1]} - MISSING\")\n",
    "        all_found = False\n",
    "\n",
    "if all_found:\n",
    "    print(\"\\nüéâ All checkpoints found!\")\n",
    "    \n",
    "    # Show metrics\n",
    "    if os.path.exists('results/metrics/spinn_structured_metrics.json'):\n",
    "        with open('results/metrics/spinn_structured_metrics.json', 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        print(f\"\\nüìä Training Summary:\")\n",
    "        print(f\"   Dense params: {metrics['pruning_history']['params'][0]:,}\")\n",
    "        print(f\"   SPINN params: {metrics['pruning_history']['params'][-1]:,}\")\n",
    "        print(f\"   Compression: {metrics['parameter_reduction']*100:.1f}%\")\n",
    "        print(f\"   Final R¬≤: {metrics['final']['overall']['r2']:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìå Next: Run Cell 13 to copy models\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Training incomplete or failed\")\n",
    "    print(\"   Check Cell 11 output for errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bb049",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 13: Copy Models to models/saved/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5cf401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"üì¶ Copying trained models to models/saved/...\\n\")\n",
    "\n",
    "# Create directory\n",
    "os.makedirs('models/saved', exist_ok=True)\n",
    "\n",
    "# Backup old models\n",
    "if os.path.exists('models/saved/dense_pinn.pth'):\n",
    "    shutil.copy('models/saved/dense_pinn.pth', 'models/saved/dense_pinn_OLD.pth')\n",
    "    print(\"‚úÖ Backed up old dense_pinn.pth\")\n",
    "\n",
    "if os.path.exists('models/saved/spinn_structured.pth'):\n",
    "    shutil.copy('models/saved/spinn_structured.pth', 'models/saved/spinn_OLD.pth')\n",
    "    print(\"‚úÖ Backed up old spinn_structured.pth\")\n",
    "\n",
    "# Copy new models\n",
    "shutil.copy('results/checkpoints/dense_pinn_improved_final.pt', \n",
    "            'models/saved/dense_pinn.pth')\n",
    "print(\"\\n‚úÖ Copied: dense_pinn.pth\")\n",
    "\n",
    "shutil.copy('results/checkpoints/spinn_structured_final.pt', \n",
    "            'models/saved/spinn_structured.pth')\n",
    "print(\"‚úÖ Copied: spinn_structured.pth\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nüìã Models in models/saved/:\")\n",
    "for model in ['dense_pinn.pth', 'spinn_structured.pth']:\n",
    "    path = f'models/saved/{model}'\n",
    "    if os.path.exists(path):\n",
    "        size = os.path.getsize(path) / (1024**2)\n",
    "        print(f\"   ‚úÖ {model} ({size:.2f} MB)\")\n",
    "\n",
    "print(\"\\nüéâ Models ready!\")\n",
    "print(\"üìå Next: Continue to Cell 14 for online adaptation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fa9c1",
   "metadata": {},
   "source": [
    "---\n",
    "# PART C: ONLINE ADAPTATION EXPERIMENT\n",
    "\n",
    "## Cell 14: Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb885132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nüìä Loading data...\")\n",
    "train_df = pd.read_csv('data/processed/train.csv')\n",
    "val_df = pd.read_csv('data/processed/val.csv')\n",
    "test_df = pd.read_csv('data/processed/test.csv')\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_df)} samples\")\n",
    "print(f\"‚úÖ Val: {len(val_df)} samples\")\n",
    "print(f\"‚úÖ Test: {len(test_df)} samples\")\n",
    "\n",
    "# Detect target columns\n",
    "print(f\"\\nüîç Detecting target columns...\")\n",
    "all_cols = test_df.columns.tolist()\n",
    "\n",
    "target_options = [\n",
    "    ['tool_wear', 'thermal_displacement'],\n",
    "    ['flank_wear', 'thermal_displacement'],\n",
    "    ['wear', 'VB'],\n",
    "    ['y1', 'y2']\n",
    "]\n",
    "\n",
    "target_cols = None\n",
    "for option in target_options:\n",
    "    if all(col in all_cols for col in option):\n",
    "        target_cols = option\n",
    "        break\n",
    "\n",
    "if target_cols is None:\n",
    "    target_cols = all_cols[-2:]\n",
    "\n",
    "print(f\"‚úÖ Target columns: {target_cols}\")\n",
    "\n",
    "# Prepare tensors\n",
    "X_test = torch.FloatTensor(test_df.drop(columns=target_cols).values).to(device)\n",
    "y_test = torch.FloatTensor(test_df[target_cols].values).to(device)\n",
    "\n",
    "print(f\"\\nüìê Data shape:\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "print(f\"   y_test: {y_test.shape}\")\n",
    "print(f\"   Features: {X_test.shape[1]}\")\n",
    "print(f\"   Targets: {y_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc1ddf",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 15: Load Pre-trained Models\n",
    "\n",
    "**üéØ CHECK COMPRESSION RATIO HERE!**\n",
    "\n",
    "You should see:\n",
    "- Dense: 666,882 params (if retrained) OR 665,346 params (old)\n",
    "- SPINN: 210,364 params (68.5%) OR 373,614 params (43.8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1473c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class DensePINN(nn.Module):\n",
    "    def __init__(self, input_dim=29, hidden_dims=[512, 512, 512, 256], output_dim=2):\n",
    "        super(DensePINN, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Get number of features\n",
    "num_features = X_test.shape[1]\n",
    "print(f\"üîß Detected {num_features} input features\\n\")\n",
    "\n",
    "# Load Dense PINN\n",
    "print(\"üì¶ Loading Dense PINN...\")\n",
    "# Try models/saved first, fall back to results/checkpoints\n",
    "try:\n",
    "    checkpoint = torch.load('models/saved/dense_pinn.pth', \n",
    "                            map_location=device, weights_only=False)\n",
    "except:\n",
    "    checkpoint = torch.load('results/checkpoints/dense_pinn_improved_final.pt', \n",
    "                            map_location=device, weights_only=False)\n",
    "\n",
    "# Load state dict\n",
    "if isinstance(checkpoint, dict):\n",
    "    dense_model = DensePINN(input_dim=num_features).to(device)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        dense_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        dense_model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    dense_model = checkpoint.to(device)\n",
    "\n",
    "dense_model.eval()\n",
    "dense_params = sum(p.numel() for p in dense_model.parameters())\n",
    "print(f\"‚úÖ Dense: {dense_params:,} total parameters\")\n",
    "\n",
    "# Load SPINN (STRUCTURED PRUNED MODEL - physically smaller architecture)\n",
    "print(\"\\nüì¶ Loading SPINN...\")\n",
    "# Load from results/checkpoints (the trained structured pruned model)\n",
    "try:\n",
    "    spinn_model = torch.load('models/saved/spinn_structured.pth', \n",
    "                            map_location=device, weights_only=False)\n",
    "except:\n",
    "    spinn_model = torch.load('results/checkpoints/spinn_structured_final.pt', \n",
    "                            map_location=device, weights_only=False)\n",
    "\n",
    "# Ensure model is on correct device\n",
    "spinn_model = spinn_model.to(device)\n",
    "spinn_model.eval()\n",
    "\n",
    "# Get architecture info\n",
    "linear_layers = [m for m in spinn_model.modules() if isinstance(m, nn.Linear)]\n",
    "hidden_dims = [layer.out_features for layer in linear_layers[:-1]]\n",
    "\n",
    "# Count parameters (all are active - no zeros in structured pruning)\n",
    "spinn_params = sum(p.numel() for p in spinn_model.parameters())\n",
    "\n",
    "print(f\"‚úÖ SPINN: {spinn_params:,} parameters\")\n",
    "print(f\"   Architecture: {hidden_dims}\")\n",
    "print(f\"   üí° Structured pruning: physically smaller model\")\n",
    "\n",
    "# Calculate compression\n",
    "compression = (1 - spinn_params / dense_params) * 100\n",
    "print(f\"\\nüéØ COMPRESSION: {compression:.1f}%\")\n",
    "print(f\"   (Structural - model is physically smaller)\")\n",
    "\n",
    "if compression > 60:\n",
    "    print(\"   ‚úÖ NEW MODELS (68.5% compression with structured pruning)\")\n",
    "elif compression > 40:\n",
    "    print(\"   ‚ö†Ô∏è OLD MODELS (43.8% compression)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Unexpected compression ratio!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd1d54",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 16: Prepare Data Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c606cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test set into 5 batches\n",
    "num_batches = 5\n",
    "batch_size = len(test_df) // num_batches\n",
    "\n",
    "print(f\"üîÑ Creating {num_batches} data batches...\\n\")\n",
    "\n",
    "new_data_batches = []\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = start_idx + batch_size if i < num_batches - 1 else len(test_df)\n",
    "    \n",
    "    batch_df = test_df.iloc[start_idx:end_idx]\n",
    "    X_batch = torch.FloatTensor(batch_df.drop(columns=target_cols).values).to(device)\n",
    "    y_batch = torch.FloatTensor(batch_df[target_cols].values).to(device)\n",
    "    \n",
    "    new_data_batches.append({\n",
    "        'batch_id': i + 1,\n",
    "        'X': X_batch,\n",
    "        'y': y_batch,\n",
    "        'size': len(batch_df)\n",
    "    })\n",
    "    \n",
    "    print(f\"Batch {i+1}: {len(batch_df)} samples\")\n",
    "\n",
    "print(f\"\\n‚úÖ Batches ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073df19",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 17: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_early_layers(model, freeze_fraction=0.8):\n",
    "    \"\"\"Freeze a fraction of early layers\"\"\"\n",
    "    all_params = list(model.parameters())\n",
    "    num_to_freeze = int(len(all_params) * freeze_fraction)\n",
    "    \n",
    "    for i, param in enumerate(all_params):\n",
    "        param.requires_grad = (i >= num_to_freeze)\n",
    "    \n",
    "    return sum(p.numel() for p in all_params if p.requires_grad)\n",
    "\n",
    "def unfreeze_all_layers(model):\n",
    "    \"\"\"Unfreeze all layers\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def fine_tune_model(model, X_batch, y_batch, num_epochs=10, lr=0.001, freeze_fraction=0.0):\n",
    "    \"\"\"Fine-tune model on batch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Apply freezing\n",
    "    if freeze_fraction > 0:\n",
    "        trainable_params = freeze_early_layers(model, freeze_fraction)\n",
    "    else:\n",
    "        unfreeze_all_layers(model)\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Setup\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_batch)\n",
    "        final_loss = criterion(predictions, y_batch).item()\n",
    "        r2 = r2_score(y_batch.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'training_time': training_time,\n",
    "        'final_loss': final_loss,\n",
    "        'r2_score': r2,\n",
    "        'trainable_params': trainable_params\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15cdd8",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 18: Run Main Experiment\n",
    "\n",
    "**This is the KEY experiment!**\n",
    "\n",
    "Compares 3 scenarios:\n",
    "1. Baseline: No adaptation\n",
    "2. Full Retrain: Update all parameters\n",
    "3. Online Adapt: Freeze 85%, update 15%\n",
    "\n",
    "‚è±Ô∏è Takes ~5 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config - UPDATED: Fewer epochs to prevent overfitting on small batches\n",
    "NUM_EPOCHS = 3  # Reduced from 10 to prevent overfitting\n",
    "LEARNING_RATE = 0.0005  # Reduced LR for more stable fine-tuning\n",
    "FREEZE_FRACTION = 0.85\n",
    "\n",
    "print(\"üöÄ ONLINE ADAPTATION EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Config: {NUM_EPOCHS} epochs, LR={LEARNING_RATE}, freeze {FREEZE_FRACTION*100:.0f}%\")\n",
    "print(\"‚ö†Ô∏è  Updated: Reduced epochs to prevent overfitting on small batches\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = {\n",
    "    'baseline': [],\n",
    "    'full_retrain': [],\n",
    "    'online_adapt': []\n",
    "}\n",
    "\n",
    "# Scenario 1: Baseline (no adaptation)\n",
    "print(\"\\nüìä Scenario 1: Baseline (No Adaptation)\")\n",
    "print(\"-\" * 70)\n",
    "spinn_baseline = deepcopy(spinn_model)\n",
    "spinn_baseline.eval()\n",
    "\n",
    "for batch in new_data_batches:\n",
    "    with torch.no_grad():\n",
    "        predictions = spinn_baseline(batch['X'])\n",
    "        loss = nn.MSELoss()(predictions, batch['y']).item()\n",
    "        r2 = r2_score(batch['y'].cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    results['baseline'].append({\n",
    "        'batch_id': batch['batch_id'],\n",
    "        'r2_score': r2,\n",
    "        'loss': loss,\n",
    "        'training_time': 0.0,\n",
    "        'trainable_params': 0\n",
    "    })\n",
    "    print(f\"Batch {batch['batch_id']}: R¬≤ = {r2:.4f}\")\n",
    "\n",
    "# Scenario 2: Full Retraining\n",
    "print(\"\\nüìä Scenario 2: Full Retraining (All Parameters)\")\n",
    "print(\"-\" * 70)\n",
    "spinn_full = deepcopy(spinn_model)\n",
    "\n",
    "for batch in new_data_batches:\n",
    "    metrics = fine_tune_model(spinn_full, batch['X'], batch['y'],\n",
    "                             NUM_EPOCHS, LEARNING_RATE, freeze_fraction=0.0)\n",
    "    \n",
    "    results['full_retrain'].append({\n",
    "        'batch_id': batch['batch_id'],\n",
    "        'r2_score': metrics['r2_score'],\n",
    "        'loss': metrics['final_loss'],\n",
    "        'training_time': metrics['training_time'],\n",
    "        'trainable_params': metrics['trainable_params']\n",
    "    })\n",
    "    print(f\"Batch {batch['batch_id']}: R¬≤ = {metrics['r2_score']:.4f}, \"\n",
    "          f\"Time = {metrics['training_time']:.2f}s\")\n",
    "\n",
    "# Scenario 3: Online Adaptation\n",
    "print(\"\\nüìä Scenario 3: Online Adaptation (Freeze 85%)\")\n",
    "print(\"-\" * 70)\n",
    "spinn_adapt = deepcopy(spinn_model)\n",
    "\n",
    "for batch in new_data_batches:\n",
    "    metrics = fine_tune_model(spinn_adapt, batch['X'], batch['y'],\n",
    "                             NUM_EPOCHS, LEARNING_RATE, freeze_fraction=FREEZE_FRACTION)\n",
    "    \n",
    "    results['online_adapt'].append({\n",
    "        'batch_id': batch['batch_id'],\n",
    "        'r2_score': metrics['r2_score'],\n",
    "        'loss': metrics['final_loss'],\n",
    "        'training_time': metrics['training_time'],\n",
    "        'trainable_params': metrics['trainable_params']\n",
    "    })\n",
    "    print(f\"Batch {batch['batch_id']}: R¬≤ = {metrics['r2_score']:.4f}, \"\n",
    "          f\"Time = {metrics['training_time']:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Experiment complete!\")\n",
    "print(\"\\nüí° Note: Using 3 epochs instead of 10 to prevent overfitting on small batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38a4a5",
   "metadata": {},
   "source": [
    "---\n",
    "# PART D: RESULTS\n",
    "\n",
    "## Cell 24: Analyze Results\n",
    "\n",
    "**This calculates your paper metrics!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a190b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "total_time_full = sum(r['training_time'] for r in results['full_retrain'])\n",
    "total_time_adapt = sum(r['training_time'] for r in results['online_adapt'])\n",
    "\n",
    "avg_r2_baseline = np.mean([r['r2_score'] for r in results['baseline']])\n",
    "avg_r2_full = np.mean([r['r2_score'] for r in results['full_retrain']])\n",
    "avg_r2_adapt = np.mean([r['r2_score'] for r in results['online_adapt']])\n",
    "\n",
    "params_full = results['full_retrain'][0]['trainable_params']\n",
    "params_adapt = results['online_adapt'][0]['trainable_params']\n",
    "\n",
    "time_reduction = (1 - total_time_adapt / total_time_full) * 100\n",
    "param_reduction = (1 - params_adapt / params_full) * 100\n",
    "computational_efficiency = (total_time_adapt / total_time_full) * 100\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìâ COMPUTATIONAL SAVINGS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Performance:\")\n",
    "print(f\"  Baseline:        R¬≤ = {avg_r2_baseline:.4f}\")\n",
    "print(f\"  Full Retrain:    R¬≤ = {avg_r2_full:.4f}\")\n",
    "print(f\"  Online Adapt:    R¬≤ = {avg_r2_adapt:.4f}\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  Training Time:\")\n",
    "print(f\"  Full Retrain:    {total_time_full:.2f}s\")\n",
    "print(f\"  Online Adapt:    {total_time_adapt:.2f}s\")\n",
    "print(f\"  Time Savings:    {time_reduction:.1f}%\")\n",
    "\n",
    "print(\"\\nüî¢ Parameters:\")\n",
    "print(f\"  Full Retrain:    {params_full:,}\")\n",
    "print(f\"  Online Adapt:    {params_adapt:,}\")\n",
    "print(f\"  Param Savings:   {param_reduction:.1f}%\")\n",
    "\n",
    "print(\"\\nüí∞ Computational Efficiency:\")\n",
    "print(f\"  Online adaptation requires {computational_efficiency:.1f}% of resources\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚ú® KEY FINDING FOR PAPER:\")\n",
    "print(f\"   Online adaptation achieves R¬≤ = {avg_r2_adapt:.4f}\")\n",
    "print(f\"   while using only {computational_efficiency:.1f}% of computational\")\n",
    "print(f\"   resources compared to full retraining\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424a76d",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 25: Generate Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1151adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 4-panel figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Online Adaptation vs Full Retraining', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Panel 1: R¬≤ progression\n",
    "ax1 = axes[0, 0]\n",
    "batches = [r['batch_id'] for r in results['baseline']]\n",
    "ax1.plot(batches, [r['r2_score'] for r in results['baseline']], \n",
    "         'o-', label='Baseline', linewidth=2, markersize=8)\n",
    "ax1.plot(batches, [r['r2_score'] for r in results['full_retrain']], \n",
    "         's-', label='Full Retrain', linewidth=2, markersize=8)\n",
    "ax1.plot(batches, [r['r2_score'] for r in results['online_adapt']], \n",
    "         '^-', label='Online Adapt', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Batch')\n",
    "ax1.set_ylabel('R¬≤ Score')\n",
    "ax1.set_title('(a) Prediction Accuracy', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Training time\n",
    "ax2 = axes[0, 1]\n",
    "times_full = [r['training_time'] for r in results['full_retrain']]\n",
    "times_adapt = [r['training_time'] for r in results['online_adapt']]\n",
    "x = np.arange(len(batches))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, times_full, width, label='Full Retrain', alpha=0.8)\n",
    "ax2.bar(x + width/2, times_adapt, width, label='Online Adapt', alpha=0.8)\n",
    "ax2.set_xlabel('Batch')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('(b) Training Time', fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(batches)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 3: Parameters\n",
    "ax3 = axes[1, 0]\n",
    "strategies = ['Full Retrain', 'Online Adapt']\n",
    "params = [params_full, params_adapt]\n",
    "bars = ax3.bar(strategies, params, color=['#1f77b4', '#ff7f0e'], alpha=0.8)\n",
    "ax3.set_ylabel('Trainable Parameters')\n",
    "ax3.set_title('(c) Computational Cost', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:,}\\n({val/params_full*100:.0f}%)',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Panel 4: Summary\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "summary = f\"\"\"\n",
    "COMPUTATIONAL SAVINGS\n",
    "\n",
    "R¬≤ Scores:\n",
    "  Baseline:      {avg_r2_baseline:.4f}\n",
    "  Full Retrain:  {avg_r2_full:.4f}\n",
    "  Online Adapt:  {avg_r2_adapt:.4f}\n",
    "\n",
    "Resource Reduction:\n",
    "  Time:         {time_reduction:.1f}% faster\n",
    "  Parameters:   {param_reduction:.1f}% fewer\n",
    "\n",
    "Key Finding:\n",
    "  {computational_efficiency:.1f}% of resources\n",
    "  Comparable accuracy\n",
    "\"\"\"\n",
    "ax4.text(0.1, 0.5, summary, transform=ax4.transAxes,\n",
    "         fontsize=11, verticalalignment='center',\n",
    "         fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "os.makedirs('results/figures', exist_ok=True)\n",
    "plt.savefig('results/figures/online_adaptation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Figure saved: results/figures/online_adaptation_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a81098",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 26: Save Results and Generate Paper Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "experiment_results = {\n",
    "    'configuration': {\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'freeze_fraction': FREEZE_FRACTION,\n",
    "        'num_batches': len(new_data_batches)\n",
    "    },\n",
    "    'detailed_results': results,\n",
    "    'summary': {\n",
    "        'avg_r2_baseline': float(avg_r2_baseline),\n",
    "        'avg_r2_full_retrain': float(avg_r2_full),\n",
    "        'avg_r2_online_adapt': float(avg_r2_adapt),\n",
    "        'total_time_full': float(total_time_full),\n",
    "        'total_time_adapt': float(total_time_adapt),\n",
    "        'time_reduction_percent': float(time_reduction),\n",
    "        'trainable_params_full': int(params_full),\n",
    "        'trainable_params_adapt': int(params_adapt),\n",
    "        'param_reduction_percent': float(param_reduction),\n",
    "        'computational_efficiency_percent': float(computational_efficiency)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "os.makedirs('results', exist_ok=True)\n",
    "with open('results/online_adaptation_results.json', 'w') as f:\n",
    "    json.dump(experiment_results, f, indent=2)\n",
    "\n",
    "print(\"üíæ Results saved: results/online_adaptation_results.json\\n\")\n",
    "\n",
    "# Paper text\n",
    "print(\"=\" * 70)\n",
    "print(\"üìÑ PAPER-READY SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFor Abstract/Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\"\"\n",
    "Online adaptation experiments demonstrate that the pruned SPINN model\n",
    "can be efficiently fine-tuned on new cutting data by freezing {FREEZE_FRACTION*100:.0f}%\n",
    "of early layers and updating only {(1-FREEZE_FRACTION)*100:.0f}% of parameters. This\n",
    "approach achieves comparable prediction accuracy (R¬≤ = {avg_r2_adapt:.4f}) to\n",
    "full retraining (R¬≤ = {avg_r2_full:.4f}) while requiring only {computational_efficiency:.1f}%\n",
    "of computational resources ({time_reduction:.1f}% time reduction, {param_reduction:.1f}%\n",
    "fewer trainable parameters). This validates the feasibility of continuous\n",
    "model updates in production environments with minimal computational overhead.\n",
    "\"\"\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\\n‚úÖ ALL RESULTS COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e18179",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ COMPLETE!\n",
    "\n",
    "## What You Accomplished:\n",
    "\n",
    "1. ‚úÖ **Model Compression:** 68.5% parameter reduction (or 43.8% if using old models)\n",
    "2. ‚úÖ **Online Adaptation:** Validated computational efficiency\n",
    "3. ‚úÖ **Experimental Results:** Generated quantitative data\n",
    "4. ‚úÖ **Publication Figure:** 4-panel analysis saved\n",
    "5. ‚úÖ **Paper Text:** Ready-to-use summary\n",
    "\n",
    "## Files Generated:\n",
    "\n",
    "- `results/figures/online_adaptation_analysis.png` - Main figure\n",
    "- `results/online_adaptation_results.json` - Complete data\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. Download the figure and JSON from Colab\n",
    "2. Update your paper with the exact percentages\n",
    "3. Add figure to manuscript\n",
    "4. Write methodology section\n",
    "\n",
    "## Gap 5 Status: ‚úÖ COMPLETE\n",
    "\n",
    "You now have full experimental validation for online adaptation! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
