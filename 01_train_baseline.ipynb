{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfe4bb8",
   "metadata": {},
   "source": [
    "# SPINN (Sparse Physics-Informed Neural Network) for CNC Milling Digital Twin\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This notebook implements a **Sparse Physics-Informed Neural Network** for real-time tool wear and thermal displacement prediction in CNC milling operations.\n",
    "\n",
    "**Key Contributions:**\n",
    "- ‚úÖ 70% parameter reduction through structured pruning\n",
    "- ‚úÖ <2% prediction error on tool wear and thermal displacement\n",
    "- ‚úÖ Physics-informed constraints (Archard wear, thermal energy conservation)\n",
    "- ‚úÖ Online adaptation capability\n",
    "- ‚úÖ Real-time inference on edge hardware\n",
    "\n",
    "**Paper Target:** ASME MSEC 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd499e",
   "metadata": {},
   "source": [
    "## üìã Setup Instructions\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "Run the cell below to install all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for SPINN project\"\"\"\n",
    "    packages = [\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'numpy',\n",
    "        'pandas',\n",
    "        'scipy',\n",
    "        'scikit-learn',\n",
    "        'matplotlib',\n",
    "        'seaborn',\n",
    "        'plotly',\n",
    "        'tqdm',\n",
    "        'pyyaml',\n",
    "        'h5py',\n",
    "        'requests'\n",
    "    ]\n",
    "    \n",
    "    print(\"üì¶ Installing packages...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"‚úÖ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"‚¨áÔ∏è  Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úÖ {package} installed\")\n",
    "\n",
    "# Run installation\n",
    "install_packages()\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e010e1",
   "metadata": {},
   "source": [
    "### Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ae907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check PyTorch and device\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\n‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47700d",
   "metadata": {},
   "source": [
    "## üìä Step 3: Download and Prepare Data\n",
    "\n",
    "### ‚ö†Ô∏è IMPORTANT: YOU NEED TO DOWNLOAD DATASETS FIRST\n",
    "\n",
    "Before running the cells below, you must:\n",
    "\n",
    "1. **Download NASA Milling Dataset**\n",
    "   - Go to: https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/\n",
    "   - Download the \"Milling Data Set\"\n",
    "   - Place files in: `data/raw/nasa/`\n",
    "\n",
    "2. **Download PHM 2010 Dataset (Optional)**\n",
    "   - Search for \"PHM Society 2010 Data Challenge\"\n",
    "   - Place files in: `data/raw/phm/`\n",
    "\n",
    "See `DATASET_INSTRUCTIONS.md` for detailed steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12654e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data exists\n",
    "import os\n",
    "\n",
    "data_dir = Path(\"data/raw/nasa\")\n",
    "data_exists = data_dir.exists() and len(list(data_dir.glob(\"*.csv\"))) > 0\n",
    "\n",
    "if data_exists:\n",
    "    print(f\"‚úÖ Found {len(list(data_dir.glob('*.csv')))} data files in {data_dir}\")\n",
    "    print(\"\\nüìÇ Files found:\")\n",
    "    for f in list(data_dir.glob(\"*.csv\"))[:5]:\n",
    "        print(f\"   - {f.name}\")\n",
    "else:\n",
    "    print(\"‚ùå No data files found!\")\n",
    "    print(f\"\\nüì• Please download the NASA Milling Dataset and place it in: {data_dir.absolute()}\")\n",
    "    print(\"\\nSee DATASET_INSTRUCTIONS.md for detailed instructions.\")\n",
    "    print(\"\\nOnce downloaded, run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd51e8",
   "metadata": {},
   "source": [
    "## üîß Step 4: Data Preprocessing\n",
    "\n",
    "This step will:\n",
    "- Load raw CSV files\n",
    "- Extract relevant features (forces, wear, process parameters)\n",
    "- Create derived features (material removal rate, thermal estimates)\n",
    "- Split into train/val/test sets\n",
    "- Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data preprocessing\n",
    "!python data/preprocess.py\n",
    "\n",
    "# Load processed data\n",
    "train_df = pd.read_csv(\"data/processed/train.csv\")\n",
    "val_df = pd.read_csv(\"data/processed/val.csv\")\n",
    "test_df = pd.read_csv(\"data/processed/test.csv\")\n",
    "\n",
    "print(f\"\\nüìä Data loaded:\")\n",
    "print(f\"   Train: {len(train_df)} samples\")\n",
    "print(f\"   Val:   {len(val_df)} samples\")\n",
    "print(f\"   Test:  {len(test_df)} samples\")\n",
    "print(f\"\\nüìã Features ({len(train_df.columns)}):\")\n",
    "for col in train_df.columns:\n",
    "    print(f\"   - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccdceb",
   "metadata": {},
   "source": [
    "## üìà Step 5: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tool wear progression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Tool wear over time\n",
    "axes[0].plot(train_df['time'], train_df['tool_wear'], alpha=0.6, label='Training data')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Tool Wear (Œºm)')\n",
    "axes[0].set_title('Tool Wear Progression')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Thermal displacement over time\n",
    "axes[1].plot(train_df['time'], train_df['thermal_displacement'], alpha=0.6, color='orange', label='Training data')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Thermal Displacement (Œºm)')\n",
    "axes[1].set_title('Thermal Displacement Progression')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/data_exploration.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Figure saved to: results/figures/data_exploration.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = train_df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Figure saved to: results/figures/correlation_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5535e2",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 6: Build Dense PINN (Baseline)\n",
    "\n",
    "Now we'll create the baseline Dense Physics-Informed Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model classes\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from models.dense_pinn import create_dense_pinn\n",
    "from models.physics_losses import CombinedLoss\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "def prepare_torch_data(df):\n",
    "    \"\"\"Convert DataFrame to PyTorch tensors\"\"\"\n",
    "    # Select input features (exclude targets and identifiers)\n",
    "    feature_cols = [col for col in df.columns \n",
    "                   if col not in ['tool_wear', 'thermal_displacement', 'experiment_id']]\n",
    "    \n",
    "    X = torch.FloatTensor(df[feature_cols].values)\n",
    "    y_wear = torch.FloatTensor(df['tool_wear'].values).unsqueeze(1)\n",
    "    y_thermal = torch.FloatTensor(df['thermal_displacement'].values).unsqueeze(1)\n",
    "    y = torch.cat([y_wear, y_thermal], dim=1)\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "X_train, y_train, feature_names = prepare_torch_data(train_df)\n",
    "X_val, y_val, _ = prepare_torch_data(val_df)\n",
    "X_test, y_test, _ = prepare_torch_data(test_df)\n",
    "\n",
    "print(f\"‚úÖ Data prepared for PyTorch:\")\n",
    "print(f\"   X_train shape: {X_train.shape}\")\n",
    "print(f\"   y_train shape: {y_train.shape}\")\n",
    "print(f\"   Features: {len(feature_names)}\")\n",
    "\n",
    "# Create model\n",
    "input_dim = X_train.shape[1]\n",
    "dense_model = create_dense_pinn(\n",
    "    input_dim=input_dim,\n",
    "    architecture='standard',\n",
    "    activation='tanh'\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nüèóÔ∏è  Dense PINN created:\")\n",
    "print(f\"   Parameters: {dense_model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe02573",
   "metadata": {},
   "source": [
    "## üéØ Step 7: Train Dense PINN\n",
    "\n",
    "Training with two-stage approach:\n",
    "1. **Stage 1**: Data loss only (warm-up)\n",
    "2. **Stage 2**: Data loss + Physics loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-3,\n",
    "    'stage1_epochs': 30,  # Data loss only\n",
    "    'stage2_epochs': 150,  # Data + Physics loss\n",
    "    'lambda_physics': 0.1,  # Physics loss weight\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Move data to device\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "\n",
    "# Create data loaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"‚úÖ Training configuration ready\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   Stage 1 epochs: {config['stage1_epochs']}\")\n",
    "print(f\"   Stage 2 epochs: {config['stage2_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6497c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, optimizer, criterion, epoch, use_physics=False):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for X_batch, y_batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(X_batch)\n",
    "        \n",
    "        if use_physics:\n",
    "            # Use combined loss\n",
    "            pred_dict = {\n",
    "                'wear': predictions[:, 0],\n",
    "                'thermal_displacement': predictions[:, 1]\n",
    "            }\n",
    "            target_dict = {\n",
    "                'wear': y_batch[:, 0],\n",
    "                'thermal_displacement': y_batch[:, 1]\n",
    "            }\n",
    "            # Create input dict for physics loss\n",
    "            input_dict = {\n",
    "                'force_x': X_batch[:, feature_names.index('force_x')],\n",
    "                'force_y': X_batch[:, feature_names.index('force_y')],\n",
    "                'force_z': X_batch[:, feature_names.index('force_z')],\n",
    "                'spindle_speed': X_batch[:, feature_names.index('spindle_speed')],\n",
    "                'feed_rate': X_batch[:, feature_names.index('feed_rate')],\n",
    "                'time': X_batch[:, feature_names.index('time')],\n",
    "                'force_magnitude': X_batch[:, feature_names.index('force_magnitude')]\n",
    "            }\n",
    "            loss, _ = criterion(pred_dict, target_dict, input_dict)\n",
    "        else:\n",
    "            # MSE loss only\n",
    "            loss = nn.MSELoss()(predictions, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, use_physics=False):\n",
    "    \"\"\"Validation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            if use_physics:\n",
    "                pred_dict = {\n",
    "                    'wear': predictions[:, 0],\n",
    "                    'thermal_displacement': predictions[:, 1]\n",
    "                }\n",
    "                target_dict = {\n",
    "                    'wear': y_batch[:, 0],\n",
    "                    'thermal_displacement': y_batch[:, 1]\n",
    "                }\n",
    "                input_dict = {\n",
    "                    'force_x': X_batch[:, feature_names.index('force_x')],\n",
    "                    'force_y': X_batch[:, feature_names.index('force_y')],\n",
    "                    'force_z': X_batch[:, feature_names.index('force_z')],\n",
    "                    'spindle_speed': X_batch[:, feature_names.index('spindle_speed')],\n",
    "                    'feed_rate': X_batch[:, feature_names.index('feed_rate')],\n",
    "                    'time': X_batch[:, feature_names.index('time')],\n",
    "                    'force_magnitude': X_batch[:, feature_names.index('force_magnitude')]\n",
    "                }\n",
    "                loss, _ = criterion(pred_dict, target_dict, input_dict)\n",
    "            else:\n",
    "                loss = nn.MSELoss()(predictions, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3439e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1: Train with data loss only\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 1: Training with DATA LOSS only (warm-up)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "optimizer = optim.Adam(dense_model.parameters(), lr=config['learning_rate'])\n",
    "mse_criterion = nn.MSELoss()\n",
    "\n",
    "stage1_train_losses = []\n",
    "stage1_val_losses = []\n",
    "\n",
    "for epoch in range(1, config['stage1_epochs'] + 1):\n",
    "    train_loss = train_epoch(dense_model, train_loader, optimizer, mse_criterion, epoch, use_physics=False)\n",
    "    val_loss = validate(dense_model, val_loader, mse_criterion, use_physics=False)\n",
    "    \n",
    "    stage1_train_losses.append(train_loss)\n",
    "    stage1_val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/{config['stage1_epochs']} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Stage 1 complete!\")\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save(dense_model.state_dict(), 'results/models/dense_pinn_stage1.pth')\n",
    "print(\"üíæ Saved checkpoint: dense_pinn_stage1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 2: Train with data + physics loss\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 2: Training with DATA + PHYSICS LOSS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "combined_criterion = CombinedLoss(\n",
    "    lambda_physics=config['lambda_physics'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Lower learning rate for stage 2\n",
    "optimizer = optim.Adam(dense_model.parameters(), lr=config['learning_rate'] * 0.5)\n",
    "\n",
    "stage2_train_losses = []\n",
    "stage2_val_losses = []\n",
    "\n",
    "for epoch in range(1, config['stage2_epochs'] + 1):\n",
    "    train_loss = train_epoch(dense_model, train_loader, optimizer, combined_criterion, \n",
    "                            epoch + config['stage1_epochs'], use_physics=True)\n",
    "    val_loss = validate(dense_model, val_loader, combined_criterion, use_physics=True)\n",
    "    \n",
    "    stage2_train_losses.append(train_loss)\n",
    "    stage2_val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{config['stage2_epochs']} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Stage 2 complete!\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(dense_model.state_dict(), 'results/models/dense_pinn_final.pth')\n",
    "print(\"üíæ Saved final model: dense_pinn_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0a381",
   "metadata": {},
   "source": [
    "## üìä Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "total_train_losses = stage1_train_losses + stage2_train_losses\n",
    "total_val_losses = stage1_val_losses + stage2_val_losses\n",
    "\n",
    "epochs = range(1, len(total_train_losses) + 1)\n",
    "ax.plot(epochs, total_train_losses, label='Train Loss', linewidth=2)\n",
    "ax.plot(epochs, total_val_losses, label='Val Loss', linewidth=2)\n",
    "ax.axvline(x=config['stage1_epochs'], color='red', linestyle='--', label='Stage 1‚Üí2 Transition')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Dense PINN Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/dense_pinn_training.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training curves saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3968d",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "### What's Done:\n",
    "‚úÖ Environment setup\n",
    "‚úÖ Data preprocessing\n",
    "‚úÖ Dense PINN implementation\n",
    "‚úÖ Two-stage training with physics-informed loss\n",
    "\n",
    "### What's Next:\n",
    "1. **Evaluate Dense PINN** on test set\n",
    "2. **Create SPINN** via iterative pruning (70% reduction)\n",
    "3. **Fine-tune SPINN** to recover accuracy\n",
    "4. **Benchmark** inference time and memory\n",
    "5. **Online adaptation** experiments\n",
    "6. **Generate all figures** for paper\n",
    "\n",
    "See other notebooks:\n",
    "- `02_evaluate_and_prune.ipynb` - Evaluation and pruning\n",
    "- `03_experiments.ipynb` - All experiments and benchmarks\n",
    "- `04_paper_figures.ipynb` - Generate publication figures"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
