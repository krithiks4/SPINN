{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7116b716",
   "metadata": {},
   "source": [
    "# üîß FIXED! - Data Extraction Restored\n",
    "\n",
    "**Problem:** Data extraction was broken - sensor values showed 10^25-10^30 (corrupted)\n",
    "\n",
    "**Root Cause:** Recent rewrites changed from **time-series downsampling** (original working method) to **statistical aggregation** (1 row per experiment)\n",
    "\n",
    "**Solution:** Restored original data extraction method that creates:\n",
    "- Multiple rows per experiment (downsampled time-series)\n",
    "- Derived features: `force_magnitude`, `cumulative_mrr`, `heat_generation`, `thermal_displacement`\n",
    "- Proper sensor value ranges (force_ac, force_dc, vib_table, vib_spindle)\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Execution Order (IN GOOGLE COLAB)\n",
    "\n",
    "1. **Cell 3** - ‚≠ê **CLONE/PULL REPO FIRST!** ‚¨ÖÔ∏è This gets the fixed code from GitHub\n",
    "2. **Cell 4** - Mount Google Drive ‚úÖ\n",
    "3. **Cell 5** - üÜï **RESTORED DATA LOADING** (now includes the fix!)\n",
    "4. **Cell 6** - Import libraries\n",
    "5. **Cell 7** - Define model architecture\n",
    "6. **Cell 8** - Load data tensors (updated for new format)\n",
    "7. **Cell 8B** - Sanity check (verify data looks good)\n",
    "8. **Cell 9** - Train dense baseline (15-30 min) ‚Üí Target R¬≤ ‚â• 0.95\n",
    "9. **Cell 10** - Structured pruning ‚Üí Target 70-80% reduction\n",
    "10. **Cell 11** - GPU benchmark\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Quick Start (Copy to Colab)\n",
    "\n",
    "```\n",
    "Step 1: Run Cell 3 (clone repo - gets the fix from GitHub!)\n",
    "Step 2: Run Cell 4 (mount Drive)\n",
    "Step 3: Run Cell 5 ‚Üí Should see normal sensor values (< 10)\n",
    "Step 4: Run Cell 6, 7, 8, 8B in sequence\n",
    "Step 5: Run Cell 9 (training - should achieve R¬≤ ‚â• 0.95!)\n",
    "```\n",
    "\n",
    "**‚úÖ The fix is already in GitHub** - Cell 3 will pull it automatically!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b35d3",
   "metadata": {},
   "source": [
    "# SPINN Notebook - Cell Reference Guide\n",
    "\n",
    "**Copy and paste cells as needed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a474e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Diagnostic Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72375a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç DIAGNOSTIC CHECK - CURRENT STATUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for data files\n",
    "print(\"\\nüìä DATA FILES:\")\n",
    "data_files = [\n",
    "    'data/processed/nasa_milling_processed.csv',\n",
    "    'data/raw/nasa/mill.mat',\n",
    "]\n",
    "for f in data_files:\n",
    "    exists = \"‚úÖ\" if os.path.exists(f) else \"‚ùå\"\n",
    "    print(f\"   {exists} {f}\")\n",
    "\n",
    "# Check for models\n",
    "print(\"\\nü§ñ MODEL FILES:\")\n",
    "model_files = [\n",
    "    'models/saved/dense_pinn.pth',\n",
    "    'models/saved/spinn_structured.pth',\n",
    "    'models/saved/spinn_structured_70pct.pth',\n",
    "    'models/saved/spinn_structured_80pct.pth',\n",
    "]\n",
    "for f in model_files:\n",
    "    if os.path.exists(f):\n",
    "        size_mb = os.path.getsize(f) / (1024*1024)\n",
    "        print(f\"   ‚úÖ {f} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {f}\")\n",
    "\n",
    "# Check Drive backup\n",
    "print(\"\\n‚òÅÔ∏è  GOOGLE DRIVE BACKUP:\")\n",
    "try:\n",
    "    if os.path.exists('/content/drive/MyDrive/SPINN_BACKUP'):\n",
    "        drive_files = []\n",
    "        for root, dirs, files in os.walk('/content/drive/MyDrive/SPINN_BACKUP'):\n",
    "            for file in files:\n",
    "                if file.endswith('.pth'):\n",
    "                    drive_files.append(os.path.join(root, file))\n",
    "        \n",
    "        if drive_files:\n",
    "            for f in drive_files:\n",
    "                size_mb = os.path.getsize(f) / (1024*1024)\n",
    "                print(f\"   ‚úÖ {f.replace('/content/drive/MyDrive/SPINN_BACKUP/', '')} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  No .pth files found in backup\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Drive not mounted or no backup folder\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è  Drive not accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502bf9d",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Delete Models (OPTIONAL - Only if architecture mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ‚ö†Ô∏è Change to True ONLY if you have architecture mismatch errors\n",
    "DELETE_MODELS = True  # Changed to True to delete old incompatible model\n",
    "\n",
    "if not DELETE_MODELS:\n",
    "    print(\"üõë DELETION DISABLED\")\n",
    "    print(\"   Change DELETE_MODELS = True to enable\")\n",
    "else:\n",
    "    print(\"üóëÔ∏è  DELETING OLD MODELS...\")\n",
    "    \n",
    "    model_files = [\n",
    "        'models/saved/dense_pinn.pth',\n",
    "        'models/saved/spinn_structured.pth',\n",
    "        'models/saved/spinn_structured_70pct.pth',\n",
    "        'models/saved/spinn_structured_80pct.pth',\n",
    "    ]\n",
    "    \n",
    "    deleted_count = 0\n",
    "    for f in model_files:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "            print(f\"   ‚úÖ Deleted: {f}\")\n",
    "            deleted_count += 1\n",
    "    \n",
    "    try:\n",
    "        drive_backup = '/content/drive/MyDrive/SPINN_BACKUP/models/saved/'\n",
    "        if os.path.exists(drive_backup):\n",
    "            for f in os.listdir(drive_backup):\n",
    "                if f.endswith('.pth'):\n",
    "                    os.remove(os.path.join(drive_backup, f))\n",
    "                    print(f\"   ‚úÖ Deleted Drive: {f}\")\n",
    "                    deleted_count += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if deleted_count > 0:\n",
    "        print(f\"\\n‚úÖ Deleted {deleted_count} files\")\n",
    "        print(\"‚ö†Ô∏è  IMPORTANT: Set DELETE_MODELS = False now!\")\n",
    "    else:\n",
    "        print(\"\\n   No models to delete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd2ab6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d673dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone or update repository\n",
    "if not os.path.exists('SPINN'):\n",
    "    !git clone https://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@github.com/krithiks4/SPINN.git\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    !cd SPINN && git pull\n",
    "    print(\"‚úÖ Repository updated\")\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir('SPINN')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q scipy scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af832af5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af159af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ad259",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Data Upload/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7914132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "# DELETE OLD CORRUPTED DATA\n",
    "processed_file = 'data/processed/nasa_milling_processed.csv'\n",
    "if os.path.exists(processed_file):\n",
    "    print(f\"üóëÔ∏è  Deleting old CSV: {processed_file}\")\n",
    "    os.remove(processed_file)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING NASA MILLING DATA - RESTORED ORIGINAL METHOD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Look for .mat file\n",
    "print(\"\\nüìÅ Looking for .mat file...\")\n",
    "mat_files = list(Path('data/raw').rglob('*.mat'))\n",
    "\n",
    "if not mat_files:\n",
    "    print(\"‚ùå No .mat file found. Please upload mill.mat:\")\n",
    "    uploaded = files.upload()\n",
    "    mat_file = list(uploaded.keys())[0]\n",
    "    os.makedirs('data/raw/nasa', exist_ok=True)\n",
    "    with open(f'data/raw/nasa/{mat_file}', 'wb') as f:\n",
    "        f.write(uploaded[mat_file])\n",
    "    mat_path = f'data/raw/nasa/{mat_file}'\n",
    "else:\n",
    "    mat_path = str(mat_files[0])\n",
    "\n",
    "print(f\"‚úÖ Found: {mat_path}\")\n",
    "file_size_mb = os.path.getsize(mat_path) / (1024*1024)\n",
    "print(f\"   File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "# Load .mat file\n",
    "print(f\"\\nüì¶ Loading MATLAB file...\")\n",
    "data = loadmat(mat_path)\n",
    "mill = data['mill']\n",
    "\n",
    "print(f\"   mill shape: {mill.shape}\")\n",
    "print(f\"   Detected {mill.shape[1]} experiments\")\n",
    "\n",
    "# Extract data - ORIGINAL METHOD with downsampling\n",
    "all_experiments = []\n",
    "downsample_factor = 100  # Take every 100th sample\n",
    "spindle_speed = 3000.0  # Default RPM\n",
    "\n",
    "print(f\"\\nüîÑ Processing experiments with downsampling (1/{downsample_factor})...\")\n",
    "\n",
    "for case_idx in range(mill.shape[1]):\n",
    "    try:\n",
    "        case_data = mill[0, case_idx]\n",
    "        \n",
    "        # Extract experiment info\n",
    "        case_num = int(case_data['case'][0, 0])\n",
    "        vb = float(case_data['VB'][0, 0])\n",
    "        doc = float(case_data['DOC'][0, 0])\n",
    "        feed = float(case_data['feed'][0, 0])\n",
    "        \n",
    "        # Extract sensor time-series\n",
    "        force_ac = case_data['smcAC']\n",
    "        force_dc = case_data['smcDC']\n",
    "        vib_table = case_data['vib_table']\n",
    "        vib_spindle = case_data['vib_spindle']\n",
    "        \n",
    "        n_samples = force_ac.shape[0]\n",
    "        \n",
    "        # Downsample\n",
    "        indices = np.arange(0, n_samples, downsample_factor)\n",
    "        \n",
    "        # Create DataFrame for this experiment\n",
    "        exp_df = pd.DataFrame({\n",
    "            'experiment_id': case_num,\n",
    "            'case_index': case_idx,\n",
    "            'time': indices / 1000.0,\n",
    "            'tool_wear': vb,\n",
    "            'depth_of_cut': doc,\n",
    "            'feed_rate': feed,\n",
    "            'force_ac': force_ac[indices].flatten(),\n",
    "            'force_dc': force_dc[indices].flatten(),\n",
    "            'vib_table': vib_table[indices].flatten(),\n",
    "            'vib_spindle': vib_spindle[indices].flatten(),\n",
    "        })\n",
    "        \n",
    "        # Approximate 3-axis forces\n",
    "        exp_df['force_x'] = exp_df['force_ac']\n",
    "        exp_df['force_y'] = exp_df['force_dc']\n",
    "        exp_df['force_z'] = exp_df['vib_table']\n",
    "        \n",
    "        # Add spindle speed\n",
    "        exp_df['spindle_speed'] = spindle_speed\n",
    "        \n",
    "        # Derived features\n",
    "        exp_df['force_magnitude'] = np.sqrt(\n",
    "            exp_df['force_x']**2 + \n",
    "            exp_df['force_y']**2 + \n",
    "            exp_df['force_z']**2\n",
    "        )\n",
    "        \n",
    "        exp_df['mrr'] = (exp_df['spindle_speed'] * \n",
    "                         exp_df['feed_rate'] * \n",
    "                         exp_df['depth_of_cut'])\n",
    "        \n",
    "        exp_df['cumulative_mrr'] = exp_df['mrr'].cumsum()\n",
    "        \n",
    "        exp_df['heat_generation'] = (\n",
    "            exp_df['force_magnitude'] * \n",
    "            exp_df['spindle_speed'] * 0.001\n",
    "        )\n",
    "        \n",
    "        exp_df['cumulative_heat'] = exp_df['heat_generation'].cumsum()\n",
    "        \n",
    "        # Thermal displacement\n",
    "        alpha = 11.7e-6\n",
    "        L_tool = 100\n",
    "        exp_df['thermal_displacement'] = (\n",
    "            alpha * L_tool * exp_df['cumulative_heat'] * 0.01\n",
    "        )\n",
    "        \n",
    "        all_experiments.append(exp_df)\n",
    "        \n",
    "        if (case_idx + 1) % 20 == 0:\n",
    "            print(f\"   Processed {case_idx + 1}/{mill.shape[1]} experiments...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Skipping case {case_idx + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(all_experiments)} experiments\")\n",
    "\n",
    "# Combine\n",
    "df = pd.concat(all_experiments, ignore_index=True)\n",
    "\n",
    "# Remove NaN/inf\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "# Filter tool wear > 0\n",
    "df = df[df['tool_wear'] > 0]\n",
    "\n",
    "# Cap thermal displacement\n",
    "df = df[df['thermal_displacement'] < 1.0]\n",
    "\n",
    "print(f\"\\nüìä Data Summary:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Samples: {len(df):,}\")\n",
    "print(f\"   Experiments: {df['experiment_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\n‚úÖ VB (Tool Wear) Statistics:\")\n",
    "print(f\"   Range: [{df['tool_wear'].min():.6f}, {df['tool_wear'].max():.6f}]\")\n",
    "print(f\"   Mean:  {df['tool_wear'].mean():.6f}\")\n",
    "print(f\"   Unique values: {df['tool_wear'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìä Sensor Statistics:\")\n",
    "print(f\"   force_ac:    range=[{df['force_ac'].min():.4f}, {df['force_ac'].max():.4f}]\")\n",
    "print(f\"   force_dc:    range=[{df['force_dc'].min():.4f}, {df['force_dc'].max():.4f}]\")\n",
    "print(f\"   vib_table:   range=[{df['vib_table'].min():.4f}, {df['vib_table'].max():.4f}]\")\n",
    "print(f\"   thermal_displacement: range=[{df['thermal_displacement'].min():.6f}, {df['thermal_displacement'].max():.6f}]\")\n",
    "\n",
    "# Save\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "df.to_csv(processed_file, index=False)\n",
    "print(f\"\\nüíæ Saved: {processed_file}\")\n",
    "print(f\"   {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ DATA LOADING COMPLETE - ORIGINAL METHOD RESTORED\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c554c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5A: Check Raw Data (Run this to diagnose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç CHECKING RAW DATA (.mat file)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check CSV first\n",
    "csv_path = 'data/processed/nasa_milling_processed.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df_csv = pd.read_csv(csv_path)\n",
    "    print(f\"\\nüìÑ CSV File ({csv_path}):\")\n",
    "    print(f\"   Shape: {df_csv.shape}\")\n",
    "    print(f\"   VB range: [{df_csv['VB'].min():.3f}, {df_csv['VB'].max():.3f}]\")\n",
    "    print(f\"   VB unique values: {df_csv['VB'].nunique()}\")\n",
    "    print(f\"   VB sample (first 10): {df_csv['VB'].head(10).values}\")\n",
    "    \n",
    "    if df_csv['VB'].max() == 0.0:\n",
    "        print(f\"\\n‚ùå CSV IS CORRUPTED! All VB values are 0.0\")\n",
    "        print(f\"   We need to regenerate from .mat file\")\n",
    "\n",
    "# Check .mat file\n",
    "mat_path = 'data/raw/nasa/mill.mat'\n",
    "if os.path.exists(mat_path):\n",
    "    print(f\"\\nüì¶ .MAT File ({mat_path}):\")\n",
    "    data = loadmat(mat_path)\n",
    "    print(f\"   Keys: {list(data.keys())}\")\n",
    "    \n",
    "    if 'mill' in data:\n",
    "        mill = data['mill']\n",
    "        print(f\"   mill dtype: {mill.dtype}\")\n",
    "        print(f\"   mill shape: {mill.shape}\")\n",
    "        \n",
    "        if mill.dtype.names:\n",
    "            print(f\"   Field names: {mill.dtype.names}\")\n",
    "            \n",
    "            # Check VB field\n",
    "            if 'VB' in mill.dtype.names:\n",
    "                vb_data = mill['VB'][0, 0]\n",
    "                print(f\"\\n   VB field:\")\n",
    "                print(f\"      Raw shape: {vb_data.shape}\")\n",
    "                print(f\"      Raw dtype: {vb_data.dtype}\")\n",
    "                \n",
    "                if vb_data.ndim > 1:\n",
    "                    vb_flat = vb_data.flatten()\n",
    "                else:\n",
    "                    vb_flat = vb_data\n",
    "                \n",
    "                print(f\"      Flattened shape: {vb_flat.shape}\")\n",
    "                print(f\"      Range: [{vb_flat.min():.6f}, {vb_flat.max():.6f}]\")\n",
    "                print(f\"      Mean: {vb_flat.mean():.6f}\")\n",
    "                print(f\"      Unique values: {len(np.unique(vb_flat))}\")\n",
    "                print(f\"      First 10 values: {vb_flat[:10]}\")\n",
    "                \n",
    "                if vb_flat.max() > 0:\n",
    "                    print(f\"\\n   ‚úÖ .MAT file has GOOD VB data!\")\n",
    "                else:\n",
    "                    print(f\"\\n   ‚ùå .MAT file ALSO has zero VB!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå .mat file not found at {mat_path}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DIAGNOSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5a716",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02ea7e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensePINN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.1):\n",
    "        super(DensePINN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0 and i < len(hidden_dims) - 1:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def calculate_neuron_importance(layer):\n",
    "    importance = torch.sum(torch.abs(layer.weight.data), dim=1)\n",
    "    return importance\n",
    "\n",
    "def prune_linear_layer(current_layer, next_layer, keep_ratio):\n",
    "    importance = calculate_neuron_importance(current_layer)\n",
    "    n_neurons = importance.shape[0]\n",
    "    n_keep = max(1, int(n_neurons * keep_ratio))\n",
    "    \n",
    "    _, indices = torch.topk(importance, n_keep)\n",
    "    indices = sorted(indices.tolist())\n",
    "    \n",
    "    new_current = nn.Linear(current_layer.in_features, n_keep, bias=(current_layer.bias is not None))\n",
    "    new_current.weight.data = current_layer.weight.data[indices, :]\n",
    "    if current_layer.bias is not None:\n",
    "        new_current.bias.data = current_layer.bias.data[indices]\n",
    "    \n",
    "    if next_layer is not None:\n",
    "        new_next = nn.Linear(n_keep, next_layer.out_features, bias=(next_layer.bias is not None))\n",
    "        new_next.weight.data = next_layer.weight.data[:, indices]\n",
    "        if next_layer.bias is not None:\n",
    "            new_next.bias.data = next_layer.bias.data\n",
    "    else:\n",
    "        new_next = None\n",
    "    \n",
    "    return new_current, new_next\n",
    "\n",
    "print(\"‚úÖ Model architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20b786",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Load Data Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628499f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "processed_file = 'data/processed/nasa_milling_processed.csv'\n",
    "df = pd.read_csv(processed_file)\n",
    "\n",
    "print(f\"\\nüìã Available columns: {list(df.columns)}\")\n",
    "print(f\"üìä Data shape: {df.shape}\")\n",
    "\n",
    "# Check if tool_wear exists (from restored data format)\n",
    "if 'tool_wear' not in df.columns:\n",
    "    raise ValueError(\"ERROR: 'tool_wear' column not found in data!\")\n",
    "\n",
    "# Create targets: tool_wear (primary) + thermal_displacement (auxiliary)\n",
    "if 'thermal_displacement' in df.columns:\n",
    "    target_cols = ['tool_wear', 'thermal_displacement']\n",
    "    print(f\"‚úÖ Using targets: tool_wear (primary) + thermal_displacement (auxiliary)\")\n",
    "else:\n",
    "    # Fallback if thermal_displacement missing\n",
    "    target_cols = ['tool_wear', 'tool_wear']\n",
    "    print(f\"‚úÖ Using targets: tool_wear (primary) + tool_wear (auxiliary)\")\n",
    "\n",
    "# Get features (exclude outputs and metadata)\n",
    "exclude_cols = ['tool_wear', 'thermal_displacement', 'experiment_id', 'case_index']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nüî¢ Features ({len(feature_cols)}): {feature_cols}\")\n",
    "print(f\"üéØ Targets ({len(target_cols)}): {target_cols}\")\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_cols].values\n",
    "\n",
    "# Check for NaN and print stats\n",
    "print(f\"\\nüìä Data Statistics:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   X NaN count: {np.isnan(X).sum()}\")\n",
    "print(f\"   y NaN count: {np.isnan(y).sum()}\")\n",
    "print(f\"   Tool wear range: [{df['tool_wear'].min():.3f}, {df['tool_wear'].max():.3f}]\")\n",
    "print(f\"   Tool wear mean: {df['tool_wear'].mean():.3f}, std: {df['tool_wear'].std():.3f}\")\n",
    "\n",
    "# Remove NaN rows\n",
    "if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "    mask = ~(np.isnan(X).any(axis=1) | np.isnan(y).any(axis=1))\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    print(f\"   ‚ö†Ô∏è Removed {(~mask).sum()} rows with NaN\")\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "# To tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = y.shape[1]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ DATA READY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")\n",
    "print(f\"Train: {X_train.shape[0]:,}, Val: {X_val.shape[0]:,}, Test: {X_test.shape[0]:,}\")\n",
    "print(f\"\\nSample normalized targets (first 5 rows):\")\n",
    "print(f\"y_train[0:5, 0] (tool_wear): {y_train[:5, 0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381103a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Train Dense Baseline (‚≠ê MAIN - 20-40 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66a05d",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8B: Quick Data Sanity Check (Run this BEFORE Cell 9!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceccd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîç DATA SANITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if data has variance\n",
    "print(f\"\\nüìä Feature variance:\")\n",
    "print(f\"   X_train variance: {X_train_tensor.var(dim=0).mean().item():.4f}\")\n",
    "print(f\"   Should be ~1.0 after normalization\")\n",
    "\n",
    "print(f\"\\nüéØ Target variance:\")\n",
    "print(f\"   y_train[:, 0] variance: {y_train_tensor[:, 0].var().item():.4f}\")\n",
    "print(f\"   y_train[:, 1] variance: {y_train_tensor[:, 1].var().item():.4f}\")\n",
    "print(f\"   Should be ~1.0 after normalization\")\n",
    "\n",
    "print(f\"\\nüìà Target statistics (VB - column 0):\")\n",
    "print(f\"   Mean: {y_train_tensor[:, 0].mean().item():.4f}\")\n",
    "print(f\"   Std:  {y_train_tensor[:, 0].std().item():.4f}\")\n",
    "print(f\"   Min:  {y_train_tensor[:, 0].min().item():.4f}\")\n",
    "print(f\"   Max:  {y_train_tensor[:, 0].max().item():.4f}\")\n",
    "\n",
    "# Quick prediction test\n",
    "print(f\"\\nüß™ Quick model test:\")\n",
    "test_model = DensePINN(input_dim, [128, 64], output_dim, dropout=0.0).to(device)\n",
    "test_output = test_model(X_train_tensor[:10])\n",
    "print(f\"   Input shape: {X_train_tensor[:10].shape}\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "print(f\"   Output sample: {test_output[0].detach().cpu().numpy()}\")\n",
    "\n",
    "# Calculate baseline R¬≤\n",
    "print(f\"\\nüìä Baseline R¬≤ (predicting mean):\")\n",
    "mean_pred = y_val_tensor[:, 0].mean()\n",
    "baseline_r2 = r2_score(\n",
    "    y_val_tensor[:, 0].cpu().numpy(), \n",
    "    torch.full_like(y_val_tensor[:, 0], mean_pred).cpu().numpy()\n",
    ")\n",
    "print(f\"   R¬≤ when always predicting mean: {baseline_r2:.4f}\")\n",
    "print(f\"   Your model MUST beat this!\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ If all checks pass, proceed to Cell 9\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3064f97",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8C: DEEP DIAGNOSTIC - Why is R¬≤ stuck at 0.09?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî¨ DEEP DIAGNOSTIC - DATA QUALITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Check feature correlations with target\n",
    "print(\"\\nüìä FEATURE CORRELATIONS WITH VB:\")\n",
    "processed_file = 'data/processed/nasa_milling_processed.csv'\n",
    "df_check = pd.read_csv(processed_file)\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in df_check.columns if col not in ['VB', 'time', 'case', 'run']]\n",
    "print(f\"Features: {feature_cols}\")\n",
    "\n",
    "correlations = []\n",
    "for col in feature_cols:\n",
    "    corr = df_check[col].corr(df_check['VB'])\n",
    "    correlations.append((col, corr))\n",
    "    print(f\"   {col:15s}: {corr:+.4f}\")\n",
    "\n",
    "# 2. Check if features have variance\n",
    "print(f\"\\nüìà FEATURE VARIANCE (raw data):\")\n",
    "for col in feature_cols:\n",
    "    var = df_check[col].var()\n",
    "    mean = df_check[col].mean()\n",
    "    print(f\"   {col:15s}: mean={mean:8.3f}, var={var:8.3f}\")\n",
    "\n",
    "# 3. Check target distribution\n",
    "print(f\"\\nüéØ VB TARGET DISTRIBUTION:\")\n",
    "print(f\"   Min:     {df_check['VB'].min():.6f}\")\n",
    "print(f\"   Max:     {df_check['VB'].max():.6f}\")\n",
    "print(f\"   Mean:    {df_check['VB'].mean():.6f}\")\n",
    "print(f\"   Median:  {df_check['VB'].median():.6f}\")\n",
    "print(f\"   Std:     {df_check['VB'].std():.6f}\")\n",
    "print(f\"   Unique:  {df_check['VB'].nunique()}\")\n",
    "\n",
    "# 4. Check if VB has linear relationship with ANY feature\n",
    "print(f\"\\nüîç STRONGEST CORRELATIONS:\")\n",
    "sorted_corr = sorted(correlations, key=lambda x: abs(x[1]), reverse=True)\n",
    "for col, corr in sorted_corr[:5]:\n",
    "    print(f\"   {col:15s}: {corr:+.4f}\")\n",
    "\n",
    "# 5. Check for constant features\n",
    "print(f\"\\n‚ö†Ô∏è  CONSTANT/NEAR-CONSTANT FEATURES:\")\n",
    "for col in feature_cols:\n",
    "    unique_vals = df_check[col].nunique()\n",
    "    if unique_vals <= 5:\n",
    "        print(f\"   {col:15s}: only {unique_vals} unique values\")\n",
    "        print(f\"      Values: {df_check[col].unique()[:10]}\")\n",
    "\n",
    "# 6. Test simple linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_simple = df_check[feature_cols].values\n",
    "y_simple = df_check['VB'].values\n",
    "\n",
    "# Remove NaN\n",
    "mask = ~(np.isnan(X_simple).any(axis=1) | np.isnan(y_simple))\n",
    "X_simple = X_simple[mask]\n",
    "y_simple = y_simple[mask]\n",
    "\n",
    "# Fit simple linear model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_simple, y_simple)\n",
    "y_pred_lr = lr.predict(X_simple)\n",
    "r2_linear = r2_score(y_simple, y_pred_lr)\n",
    "\n",
    "print(f\"\\nüß™ SIMPLE LINEAR REGRESSION TEST:\")\n",
    "print(f\"   R¬≤ score: {r2_linear:.4f}\")\n",
    "print(f\"   Interpretation:\")\n",
    "if r2_linear < 0.1:\n",
    "    print(f\"      ‚ùå CRITICAL: Features have almost NO linear relationship with VB!\")\n",
    "    print(f\"      This explains why neural network stuck at R¬≤~0.09\")\n",
    "    print(f\"      Possible issues:\")\n",
    "    print(f\"         1. Wrong features extracted from .mat file\")\n",
    "    print(f\"         2. VB values not properly aligned with features\")\n",
    "    print(f\"         3. Need different preprocessing or feature engineering\")\n",
    "elif r2_linear < 0.3:\n",
    "    print(f\"      ‚ö†Ô∏è  Weak linear relationship - may need more complex features\")\n",
    "else:\n",
    "    print(f\"      ‚úÖ Features have predictive power\")\n",
    "\n",
    "# 7. Feature importance from linear model\n",
    "print(f\"\\nüéØ LINEAR MODEL COEFFICIENTS (feature importance):\")\n",
    "coef_importance = [(feature_cols[i], abs(lr.coef_[i])) for i in range(len(feature_cols))]\n",
    "coef_importance = sorted(coef_importance, key=lambda x: x[1], reverse=True)\n",
    "for feat, coef in coef_importance:\n",
    "    print(f\"   {feat:15s}: {coef:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DIAGNOSTIC COMPLETE - Check results above\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "dense_model_path = 'models/saved/dense_pinn.pth'\n",
    "\n",
    "# Try to restore from Drive\n",
    "try:\n",
    "    drive_path = '/content/drive/MyDrive/SPINN_BACKUP/models/saved/dense_pinn.pth'\n",
    "    if os.path.exists(drive_path) and not os.path.exists(dense_model_path):\n",
    "        os.makedirs(os.path.dirname(dense_model_path), exist_ok=True)\n",
    "        shutil.copy(drive_path, dense_model_path)\n",
    "        print(\"üì• Restored from Drive\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Try to load existing model\n",
    "model_loaded = False\n",
    "if os.path.exists(dense_model_path):\n",
    "    try:\n",
    "        dense_model = torch.load(dense_model_path, map_location=device, weights_only=False)\n",
    "        dense_model.to(device)\n",
    "        dense_model.eval()\n",
    "        \n",
    "        # Check if architecture matches\n",
    "        test_input = X_val_tensor[:1]\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                val_pred = dense_model(test_input)\n",
    "                val_pred_full = dense_model(X_val_tensor)\n",
    "                val_r2 = r2_score(y_val_tensor[:, 0].cpu().numpy(), val_pred_full[:, 0].cpu().numpy())\n",
    "            \n",
    "            total_params = sum(p.numel() for p in dense_model.parameters())\n",
    "            print(f\"‚úÖ Model loaded: {total_params:,} params, R¬≤={val_r2:.4f}\")\n",
    "            \n",
    "            if val_r2 >= 0.90:\n",
    "                print(\"üéâ Model is good! Skipping training.\")\n",
    "                model_loaded = True\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è R¬≤ too low, will retrain\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ö†Ô∏è Architecture mismatch: {e}\")\n",
    "            print(f\"   Current data has {input_dim} features, old model incompatible\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Load failed: {e}\")\n",
    "\n",
    "if not model_loaded:\n",
    "    print(\"\\nüèãÔ∏è Training from scratch (30-50 min)...\\n\")\n",
    "    \n",
    "    # OPTIMIZED ARCHITECTURE - Balance capacity and trainability\n",
    "    dense_model = DensePINN(input_dim, [1024, 512, 512, 256, 128], output_dim, dropout=0.2).to(device)\n",
    "    total_params = sum(p.numel() for p in dense_model.parameters())\n",
    "    print(f\"Architecture: {input_dim} ‚Üí 1024 ‚Üí 512 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí {output_dim}\")\n",
    "    print(f\"Parameters: {total_params:,}\")\n",
    "    print(f\"Target: R¬≤ ‚â• 0.95 (< 2% error)\\n\")\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # MULTI-STAGE LEARNING RATE STRATEGY\n",
    "    # Stage 1: Warm-up with moderate LR\n",
    "    # Stage 2: Aggressive training with higher LR\n",
    "    # Stage 3: Fine-tuning with low LR\n",
    "    optimizer = optim.Adam(dense_model.parameters(), lr=0.002, weight_decay=5e-5)\n",
    "    \n",
    "    # Cosine annealing with restarts for better convergence\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=1e-6)\n",
    "    \n",
    "    patience_counter = 0\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    for epoch in range(500):  # Extended training with better convergence\n",
    "        dense_model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = dense_model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(dense_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Update scheduler every epoch (Cosine annealing)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluate every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            dense_model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = dense_model(X_val_tensor)\n",
    "                val_loss = loss_fn(val_pred, y_val_tensor)\n",
    "                val_r2 = r2_score(y_val_tensor[:, 0].cpu().numpy(), val_pred[:, 0].cpu().numpy())\n",
    "                # Only calculate R¬≤ on primary output (VB)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Calculate error percentage\n",
    "            error_pct = (1 - val_r2) * 100\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:3d}: TrainLoss={train_loss/len(train_loader):.6f}, ValLoss={val_loss:.6f}, R¬≤={val_r2:.4f}, Error={error_pct:.2f}%, LR={current_lr:.6f}\")\n",
    "            old_lr = optimizer.param_groups[0]['lr']\n",
    "            scheduler.step(val_r2)\n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "            if new_lr < old_lr:\n",
    "                patience_counter = 0\n",
    "                print(f\"   ‚≠ê New best R¬≤! (Error: {(1-best_r2)*100:.2f}%)\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                no_improve_epochs += 1\n",
    "            \n",
    "            # Success criteria\n",
    "            if val_r2 >= 0.98:\n",
    "                print(f\"\\nüéâ EXCELLENT! R¬≤ ‚â• 0.98 achieved (< 2% error)!\")\n",
    "                break\n",
    "            \n",
    "            if val_r2 >= 0.95 and epoch >= 100:\n",
    "                print(f\"\\n‚úÖ Target R¬≤ ‚â• 0.95 achieved!\")\n",
    "                break\n",
    "            \n",
    "            # Early stopping with patience\n",
    "            if patience_counter >= 40:\n",
    "                print(f\"\\n‚ö†Ô∏è Early stopping (no improvement for 40 checks)\")\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        dense_model.load_state_dict(best_state)\n",
    "    \n",
    "    # Final evaluation\n",
    "    dense_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = dense_model(X_val_tensor)\n",
    "        final_r2 = r2_score(y_val_tensor[:, 0].cpu().numpy(), val_pred[:, 0].cpu().numpy())\n",
    "    \n",
    "    # Save\n",
    "    os.makedirs(os.path.dirname(dense_model_path), exist_ok=True)\n",
    "    torch.save(dense_model, dense_model_path)\n",
    "    \n",
    "    try:\n",
    "        drive_path = '/content/drive/MyDrive/SPINN_BACKUP/models/saved/dense_pinn.pth'\n",
    "        os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
    "        shutil.copy(dense_model_path, drive_path)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"üíæ Saved: {dense_model_path}\")    print(f\"Parameters: {total_params:,}\")    print(f\"Final R¬≤: {final_r2:.4f}\")    print(f\"Best R¬≤: {best_r2:.4f}\")    print(f\"{'='*70}\")    print(f\"‚úÖ TRAINING COMPLETE\")    print(f\"\\n{'='*70}\")    print(f\"Best R¬≤: {best_r2:.4f}\")\n",
    "    print(f\"Final R¬≤: {final_r2:.4f}\")\n",
    "    print(f\"Parameters: {total_params:,}\")\n",
    "    print(f\"üíæ Saved: {dense_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde208c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Structured Pruning (‚≠ê 10-15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec77b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "TARGET_SPARSITY = 0.80\n",
    "N_PRUNE_ROUNDS = 4\n",
    "EPOCHS_PER_ROUND = 40\n",
    "MIN_R2_THRESHOLD = 0.93\n",
    "\n",
    "dense_params = sum(p.numel() for p in dense_model.parameters())\n",
    "keep_ratio = (1 - TARGET_SPARSITY) ** (1 / N_PRUNE_ROUNDS)\n",
    "\n",
    "spinn_model = DensePINN(input_dim, [1024, 1024, 512, 512, 256], output_dim, dropout=0.15).to(device)\n",
    "spinn_model.load_state_dict(dense_model.state_dict())\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for round_num in range(1, N_PRUNE_ROUNDS + 1):\n",
    "    print(f\"\\nüîÑ ROUND {round_num}/{N_PRUNE_ROUNDS}\")\n",
    "    \n",
    "    # Prune\n",
    "    linear_layers = [m for m in spinn_model.layers if isinstance(m, nn.Linear)]\n",
    "    new_layers = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(spinn_model.layers):\n",
    "        layer = spinn_model.layers[i]\n",
    "        \n",
    "        if isinstance(layer, nn.Linear):\n",
    "            linear_idx = linear_layers.index(layer)\n",
    "            \n",
    "            if linear_idx == 0 or linear_idx == len(linear_layers) - 1:\n",
    "                new_layers.append(layer)\n",
    "                i += 1\n",
    "            else:\n",
    "                next_linear_idx = None\n",
    "                for j in range(i + 1, len(spinn_model.layers)):\n",
    "                    if isinstance(spinn_model.layers[j], nn.Linear):\n",
    "                        next_linear_idx = j\n",
    "                        break\n",
    "                \n",
    "                if next_linear_idx:\n",
    "                    next_linear = spinn_model.layers[next_linear_idx]\n",
    "                    pruned_layer, pruned_next = prune_linear_layer(layer, next_linear, keep_ratio)\n",
    "                    \n",
    "                    new_layers.append(pruned_layer)\n",
    "                    \n",
    "                    for k in range(i + 1, next_linear_idx):\n",
    "                        intermediate = spinn_model.layers[k]\n",
    "                        if isinstance(intermediate, nn.BatchNorm1d):\n",
    "                            new_layers.append(nn.BatchNorm1d(pruned_layer.out_features))\n",
    "                        else:\n",
    "                            new_layers.append(intermediate)\n",
    "                    \n",
    "                    spinn_model.layers[next_linear_idx] = pruned_next\n",
    "                    i = next_linear_idx\n",
    "                else:\n",
    "                    new_layers.append(layer)\n",
    "                    i += 1\n",
    "        else:\n",
    "            if not any(isinstance(spinn_model.layers[j], nn.Linear) and j < i for j in range(max(0, i-3), i)):\n",
    "                new_layers.append(layer)\n",
    "            i += 1\n",
    "    \n",
    "    spinn_model = nn.Sequential(*new_layers).to(device)\n",
    "    \n",
    "    pruned_params = sum(p.numel() for p in spinn_model.parameters())\n",
    "    reduction = (1 - pruned_params / dense_params) * 100\n",
    "    print(f\"Parameters: {dense_params:,} ‚Üí {pruned_params:,} ({reduction:.1f}% reduction)\")\n",
    "    \n",
    "    # Fine-tune\n",
    "    optimizer = optim.AdamW(spinn_model.parameters(), lr=0.003, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PER_ROUND)\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(EPOCHS_PER_ROUND):\n",
    "        spinn_model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = spinn_model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(spinn_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == EPOCHS_PER_ROUND - 1:\n",
    "            spinn_model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = spinn_model(X_val_tensor)\n",
    "                val_r2 = r2_score(y_val_tensor[:, 0].cpu().numpy(), val_pred[:, 0].cpu().numpy())\n",
    "            \n",
    "            if val_r2 > best_r2:\n",
    "                best_r2 = val_r2\n",
    "                best_state = {k: v.cpu().clone() for k, v in spinn_model.state_dict().items()}\n",
    "    \n",
    "    if best_state:\n",
    "        spinn_model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    \n",
    "    print(f\"‚úÖ Best R¬≤: {best_r2:.4f}\")\n",
    "    \n",
    "    if best_r2 < MIN_R2_THRESHOLD:\n",
    "        print(f\"‚ö†Ô∏è R¬≤ < {MIN_R2_THRESHOLD}, stopping\")\n",
    "        break\n",
    "\n",
    "# Final evaluation\n",
    "spinn_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred = spinn_model(X_val_tensor)\n",
    "    final_r2 = r2_score(y_val_tensor[:, 0].cpu().numpy(), val_pred[:, 0].cpu().numpy())\n",
    "\n",
    "final_params = sum(p.numel() for p in spinn_model.parameters())\n",
    "final_reduction = (1 - final_params / dense_params) * 100\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ PRUNING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Dense:   {dense_params:,}\")\n",
    "print(f\"Pruned:  {final_params:,}\")\n",
    "print(f\"Reduction: {final_reduction:.1f}%\")\n",
    "print(f\"Final R¬≤: {final_r2:.4f}\")\n",
    "print(f\"Compression: {dense_params/final_params:.1f}x\")\n",
    "\n",
    "# Save\n",
    "spinn_path = f'models/saved/spinn_structured_{int(final_reduction)}pct.pth'\n",
    "os.makedirs(os.path.dirname(spinn_path), exist_ok=True)\n",
    "torch.save(spinn_model, spinn_path)\n",
    "\n",
    "try:\n",
    "    drive_path = f'/content/drive/MyDrive/SPINN_BACKUP/models/saved/spinn_structured_{int(final_reduction)}pct.pth'\n",
    "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
    "    shutil.copy(spinn_path, drive_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\nüíæ Saved: {spinn_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8c695",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: GPU Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276de547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GPU BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_trials = 200\n",
    "warmup = 50\n",
    "\n",
    "dense_model.eval()\n",
    "for _ in range(warmup):\n",
    "    with torch.no_grad():\n",
    "        _ = dense_model(X_val_tensor)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "dense_times = []\n",
    "for _ in range(n_trials):\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = dense_model(X_val_tensor)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        dense_times.append(start.elapsed_time(end))\n",
    "    else:\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = dense_model(X_val_tensor)\n",
    "        end = time.perf_counter()\n",
    "        dense_times.append((end - start) * 1000)\n",
    "\n",
    "dense_median = np.median(dense_times)\n",
    "\n",
    "spinn_model.eval()\n",
    "for _ in range(warmup):\n",
    "    with torch.no_grad():\n",
    "        _ = spinn_model(X_val_tensor)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "spinn_times = []\n",
    "for _ in range(n_trials):\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = spinn_model(X_val_tensor)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        spinn_times.append(start.elapsed_time(end))\n",
    "    else:\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = spinn_model(X_val_tensor)\n",
    "        end = time.perf_counter()\n",
    "        spinn_times.append((end - start) * 1000)\n",
    "\n",
    "spinn_median = np.median(spinn_times)\n",
    "speedup = dense_median / spinn_median\n",
    "\n",
    "print(f\"\\nDense:  {dense_median:.2f} ms\")\n",
    "print(f\"SPINN:  {spinn_median:.2f} ms\")\n",
    "print(f\"‚ö° SPEEDUP: {speedup:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
