{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf50561",
   "metadata": {},
   "source": [
    "# SPINN - Sparse Physics-Informed Neural Networks\n",
    "## Tool Wear Prediction for CNC Milling\n",
    "\n",
    "**Target Performance:**\n",
    "- Dense Model: R¬≤ ‚â• 0.95 (< 5% error)\n",
    "- Pruned Model: 70-80% parameter reduction with R¬≤ ‚â• 0.93\n",
    "\n",
    "**Execution Order:**\n",
    "1. Cell 1 - Diagnostic check\n",
    "2. Cell 2 - Clone repository\n",
    "3. Cell 3 - Mount Google Drive\n",
    "4. Cell 4 - Load data from .mat file\n",
    "5. Cell 5 - Import libraries\n",
    "6. Cell 6 - Define model architecture\n",
    "7. Cell 7 - Load data tensors\n",
    "8. Cell 8 - Feature engineering (boosts R¬≤ from 0.87 ‚Üí 0.95+)\n",
    "9. Cell 9 - Train dense model (30-40 min)\n",
    "10. Cell 10 - Structured pruning (10-15 min)\n",
    "11. Cell 11 - GPU benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c557952",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 1: Diagnostic Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05202259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç DIAGNOSTIC CHECK - CURRENT STATUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for data files\n",
    "print(\"\\nüìä DATA FILES:\")\n",
    "data_files = [\n",
    "    'data/processed/nasa_milling_processed.csv',\n",
    "    'data/raw/nasa/mill.mat',\n",
    "]\n",
    "for f in data_files:\n",
    "    exists = \"‚úÖ\" if os.path.exists(f) else \"‚ùå\"\n",
    "    print(f\"   {exists} {f}\")\n",
    "\n",
    "# Check for models\n",
    "print(\"\\nü§ñ MODEL FILES:\")\n",
    "model_files = [\n",
    "    'models/saved/dense_pinn.pth',\n",
    "    'models/saved/spinn_structured.pth',\n",
    "]\n",
    "for f in model_files:\n",
    "    if os.path.exists(f):\n",
    "        size_mb = os.path.getsize(f) / (1024*1024)\n",
    "        print(f\"   ‚úÖ {f} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {f}\")\n",
    "\n",
    "# Check Drive backup\n",
    "print(\"\\n‚òÅÔ∏è  GOOGLE DRIVE BACKUP:\")\n",
    "try:\n",
    "    if os.path.exists('/content/drive/MyDrive/SPINN_BACKUP'):\n",
    "        drive_files = []\n",
    "        for root, dirs, files in os.walk('/content/drive/MyDrive/SPINN_BACKUP'):\n",
    "            for file in files:\n",
    "                if file.endswith('.pth'):\n",
    "                    drive_files.append(os.path.join(root, file))\n",
    "        \n",
    "        if drive_files:\n",
    "            for f in drive_files:\n",
    "                size_mb = os.path.getsize(f) / (1024*1024)\n",
    "                print(f\"   ‚úÖ {f.replace('/content/drive/MyDrive/SPINN_BACKUP/', '')} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  No .pth files found in backup\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Drive not mounted or no backup folder\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è  Drive not accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5fc76",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 2: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone or update repository\n",
    "if not os.path.exists('SPINN'):\n",
    "    !git clone https://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@github.com/krithiks4/SPINN.git\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    !cd SPINN && git pull\n",
    "    print(\"‚úÖ Repository updated\")\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir('SPINN')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q scipy scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721cd8d",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 3: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b40e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c85bb2",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 4: Load Data from .mat File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "# Delete old data if exists\n",
    "processed_file = 'data/processed/nasa_milling_processed.csv'\n",
    "if os.path.exists(processed_file):\n",
    "    print(f\"üóëÔ∏è  Deleting old CSV: {processed_file}\")\n",
    "    os.remove(processed_file)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING NASA MILLING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Look for .mat file\n",
    "print(\"\\nüìÅ Looking for .mat file...\")\n",
    "mat_files = list(Path('data/raw').rglob('*.mat'))\n",
    "\n",
    "if not mat_files:\n",
    "    print(\"‚ùå No .mat file found. Please upload mill.mat:\")\n",
    "    uploaded = files.upload()\n",
    "    mat_file = list(uploaded.keys())[0]\n",
    "    os.makedirs('data/raw/nasa', exist_ok=True)\n",
    "    with open(f'data/raw/nasa/{mat_file}', 'wb') as f:\n",
    "        f.write(uploaded[mat_file])\n",
    "    mat_path = f'data/raw/nasa/{mat_file}'\n",
    "else:\n",
    "    mat_path = str(mat_files[0])\n",
    "\n",
    "print(f\"‚úÖ Found: {mat_path}\")\n",
    "file_size_mb = os.path.getsize(mat_path) / (1024*1024)\n",
    "print(f\"   File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "# Load .mat file\n",
    "print(f\"\\nüì¶ Loading MATLAB file...\")\n",
    "data = loadmat(mat_path)\n",
    "mill = data['mill']\n",
    "\n",
    "print(f\"   mill shape: {mill.shape}\")\n",
    "print(f\"   Detected {mill.shape[1]} experiments\")\n",
    "\n",
    "# Extract data with downsampling\n",
    "all_experiments = []\n",
    "downsample_factor = 100\n",
    "spindle_speed = 3000.0\n",
    "\n",
    "print(f\"\\nüîÑ Processing experiments with downsampling (1/{downsample_factor})...\")\n",
    "\n",
    "for case_idx in range(mill.shape[1]):\n",
    "    try:\n",
    "        case_data = mill[0, case_idx]\n",
    "        \n",
    "        # Extract experiment info\n",
    "        case_num = int(case_data['case'][0, 0])\n",
    "        vb = float(case_data['VB'][0, 0])\n",
    "        doc = float(case_data['DOC'][0, 0])\n",
    "        feed = float(case_data['feed'][0, 0])\n",
    "        \n",
    "        # Extract sensor time-series\n",
    "        force_ac = case_data['smcAC']\n",
    "        force_dc = case_data['smcDC']\n",
    "        vib_table = case_data['vib_table']\n",
    "        vib_spindle = case_data['vib_spindle']\n",
    "        \n",
    "        n_samples = force_ac.shape[0]\n",
    "        indices = np.arange(0, n_samples, downsample_factor)\n",
    "        \n",
    "        # Create DataFrame for this experiment\n",
    "        exp_df = pd.DataFrame({\n",
    "            'experiment_id': case_num,\n",
    "            'case_index': case_idx,\n",
    "            'time': indices / 1000.0,\n",
    "            'tool_wear': vb,\n",
    "            'depth_of_cut': doc,\n",
    "            'feed_rate': feed,\n",
    "            'force_ac': force_ac[indices].flatten(),\n",
    "            'force_dc': force_dc[indices].flatten(),\n",
    "            'vib_table': vib_table[indices].flatten(),\n",
    "            'vib_spindle': vib_spindle[indices].flatten(),\n",
    "        })\n",
    "        \n",
    "        # Approximate 3-axis forces\n",
    "        exp_df['force_x'] = exp_df['force_ac']\n",
    "        exp_df['force_y'] = exp_df['force_dc']\n",
    "        exp_df['force_z'] = exp_df['vib_table']\n",
    "        exp_df['spindle_speed'] = spindle_speed\n",
    "        \n",
    "        # Derived features\n",
    "        exp_df['force_magnitude'] = np.sqrt(\n",
    "            exp_df['force_x']**2 + exp_df['force_y']**2 + exp_df['force_z']**2\n",
    "        )\n",
    "        exp_df['mrr'] = exp_df['spindle_speed'] * exp_df['feed_rate'] * exp_df['depth_of_cut']\n",
    "        exp_df['cumulative_mrr'] = exp_df['mrr'].cumsum()\n",
    "        exp_df['heat_generation'] = exp_df['force_magnitude'] * exp_df['spindle_speed'] * 0.001\n",
    "        exp_df['cumulative_heat'] = exp_df['heat_generation'].cumsum()\n",
    "        \n",
    "        # Thermal displacement\n",
    "        alpha = 11.7e-6\n",
    "        L_tool = 100\n",
    "        exp_df['thermal_displacement'] = alpha * L_tool * exp_df['cumulative_heat'] * 0.01\n",
    "        \n",
    "        all_experiments.append(exp_df)\n",
    "        \n",
    "        if (case_idx + 1) % 20 == 0:\n",
    "            print(f\"   Processed {case_idx + 1}/{mill.shape[1]} experiments...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Skipping case {case_idx + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(all_experiments)} experiments\")\n",
    "\n",
    "# Combine and clean\n",
    "df = pd.concat(all_experiments, ignore_index=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "df = df[df['tool_wear'] > 0]\n",
    "df = df[df['thermal_displacement'] < 1.0]\n",
    "\n",
    "print(f\"\\nüìä Data Summary:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Samples: {len(df):,}\")\n",
    "print(f\"   Experiments: {df['experiment_id'].nunique()}\")\n",
    "print(f\"\\n‚úÖ Tool Wear Statistics:\")\n",
    "print(f\"   Range: [{df['tool_wear'].min():.6f}, {df['tool_wear'].max():.6f}]\")\n",
    "print(f\"   Mean:  {df['tool_wear'].mean():.6f}\")\n",
    "print(f\"   Unique values: {df['tool_wear'].nunique()}\")\n",
    "\n",
    "# Save\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "df.to_csv(processed_file, index=False)\n",
    "print(f\"\\nüíæ Saved: {processed_file}\")\n",
    "print(f\"   {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ DATA LOADING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c852de",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 5: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e82d5",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 6: Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94006ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensePINN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.1):\n",
    "        super(DensePINN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0 and i < len(hidden_dims) - 1:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def calculate_neuron_importance(layer):\n",
    "    importance = torch.sum(torch.abs(layer.weight.data), dim=1)\n",
    "    return importance\n",
    "\n",
    "def prune_linear_layer(current_layer, next_layer, keep_ratio):\n",
    "    importance = calculate_neuron_importance(current_layer)\n",
    "    n_neurons = importance.shape[0]\n",
    "    n_keep = max(1, int(n_neurons * keep_ratio))\n",
    "    \n",
    "    _, indices = torch.topk(importance, n_keep)\n",
    "    indices = sorted(indices.tolist())\n",
    "    \n",
    "    new_current = nn.Linear(current_layer.in_features, n_keep, bias=(current_layer.bias is not None))\n",
    "    new_current.weight.data = current_layer.weight.data[indices, :]\n",
    "    if current_layer.bias is not None:\n",
    "        new_current.bias.data = current_layer.bias.data[indices]\n",
    "    \n",
    "    if next_layer is not None:\n",
    "        new_next = nn.Linear(n_keep, next_layer.out_features, bias=(next_layer.bias is not None))\n",
    "        new_next.weight.data = next_layer.weight.data[:, indices]\n",
    "        if next_layer.bias is not None:\n",
    "            new_next.bias.data = next_layer.bias.data\n",
    "    else:\n",
    "        new_next = None\n",
    "    \n",
    "    return new_current, new_next\n",
    "\n",
    "print(\"‚úÖ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fe0d1",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 7: Load Data Tensors (Original Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA - ORIGINAL FEATURES (16 features)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "processed_file = 'data/processed/nasa_milling_processed.csv'\n",
    "df = pd.read_csv(processed_file)\n",
    "\n",
    "print(f\"\\nüìã Available columns: {list(df.columns)}\")\n",
    "print(f\"üìä Data shape: {df.shape}\")\n",
    "\n",
    "# Create targets\n",
    "target_cols = ['tool_wear', 'thermal_displacement']\n",
    "exclude_cols = ['tool_wear', 'thermal_displacement', 'experiment_id', 'case_index']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nüî¢ Features ({len(feature_cols)}): {feature_cols}\")\n",
    "print(f\"üéØ Targets ({len(target_cols)}): {target_cols}\")\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_cols].values\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "# To tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = y.shape[1]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ DATA READY (ORIGINAL)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")\n",
    "print(f\"Train: {X_train.shape[0]:,}, Val: {X_val.shape[0]:,}, Test: {X_test.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5af6d",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 8: Feature Engineering (Run to boost R¬≤ from 0.87 ‚Üí 0.95+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîß FEATURE ENGINEERING - Boost R¬≤ to ‚â• 0.95\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Reload data\n",
    "df_eng = pd.read_csv(processed_file)\n",
    "\n",
    "print(f\"\\nüìä Original: {df_eng.shape[1] - 4} features\")\n",
    "\n",
    "# 1. Interaction features\n",
    "print(f\"\\nüîÑ Adding features...\")\n",
    "df_eng['force_dc_x_time'] = df_eng['force_dc'] * df_eng['time']\n",
    "df_eng['vib_spindle_x_time'] = df_eng['vib_spindle'] * df_eng['time']\n",
    "df_eng['force_mag_x_time'] = df_eng['force_magnitude'] * df_eng['time']\n",
    "df_eng['cumul_heat_x_time'] = df_eng['cumulative_heat'] * df_eng['time']\n",
    "\n",
    "# 2. Polynomial features\n",
    "df_eng['force_dc_squared'] = df_eng['force_dc'] ** 2\n",
    "df_eng['force_dc_cubed'] = df_eng['force_dc'] ** 3\n",
    "df_eng['vib_spindle_squared'] = df_eng['vib_spindle'] ** 2\n",
    "df_eng['cumulative_heat_sq'] = df_eng['cumulative_heat'] ** 2\n",
    "\n",
    "# 3. Physics-based features\n",
    "df_eng['specific_cutting_energy'] = df_eng['force_magnitude'] / (df_eng['mrr'] + 1e-6)\n",
    "df_eng['force_dc_ac_ratio'] = df_eng['force_dc'] / (df_eng['force_ac'].abs() + 1e-6)\n",
    "df_eng['vib_ratio'] = df_eng['vib_table'] / (df_eng['vib_spindle'] + 1e-6)\n",
    "df_eng['cumulative_force'] = df_eng.groupby('experiment_id')['force_magnitude'].cumsum()\n",
    "df_eng['avg_force_history'] = df_eng.groupby('experiment_id')['force_magnitude'].expanding().mean().reset_index(drop=True)\n",
    "\n",
    "# Clean\n",
    "df_eng = df_eng.replace([np.inf, -np.inf], np.nan)\n",
    "df_eng = df_eng.dropna()\n",
    "\n",
    "print(f\"‚úÖ Enhanced: {df_eng.shape[1] - 4} features (+{(df_eng.shape[1] - 4) - 16} new)\")\n",
    "\n",
    "# Prepare tensors\n",
    "feature_cols_eng = [col for col in df_eng.columns if col not in exclude_cols]\n",
    "X_eng = df_eng[feature_cols_eng].values\n",
    "y_eng = df_eng[target_cols].values\n",
    "\n",
    "# Split\n",
    "X_temp_eng, X_test_eng, y_temp_eng, y_test_eng = train_test_split(X_eng, y_eng, test_size=0.15, random_state=42)\n",
    "X_train_eng, X_val_eng, y_train_eng, y_val_eng = train_test_split(X_temp_eng, y_temp_eng, test_size=0.176, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "scaler_X_eng = StandardScaler()\n",
    "scaler_y_eng = StandardScaler()\n",
    "\n",
    "X_train_eng = scaler_X_eng.fit_transform(X_train_eng)\n",
    "X_val_eng = scaler_X_eng.transform(X_val_eng)\n",
    "X_test_eng = scaler_X_eng.transform(X_test_eng)\n",
    "\n",
    "y_train_eng = scaler_y_eng.fit_transform(y_train_eng)\n",
    "y_val_eng = scaler_y_eng.transform(y_val_eng)\n",
    "y_test_eng = scaler_y_eng.transform(y_test_eng)\n",
    "\n",
    "# To tensors\n",
    "X_train_tensor_eng = torch.FloatTensor(X_train_eng).to(device)\n",
    "y_train_tensor_eng = torch.FloatTensor(y_train_eng).to(device)\n",
    "X_val_tensor_eng = torch.FloatTensor(X_val_eng).to(device)\n",
    "y_val_tensor_eng = torch.FloatTensor(y_val_eng).to(device)\n",
    "X_test_tensor_eng = torch.FloatTensor(X_test_eng).to(device)\n",
    "y_test_tensor_eng = torch.FloatTensor(y_test_eng).to(device)\n",
    "\n",
    "input_dim_eng = X_eng.shape[1]\n",
    "output_dim_eng = y_eng.shape[1]\n",
    "\n",
    "# Test linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_eng = LinearRegression()\n",
    "lr_eng.fit(X_train_eng, y_train_eng[:, 0])\n",
    "y_pred_lr_eng = lr_eng.predict(X_val_eng)\n",
    "r2_linear_eng = r2_score(y_val_eng[:, 0], y_pred_lr_eng)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ ENHANCED DATA READY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Features: 16 ‚Üí {input_dim_eng} (+{input_dim_eng - 16})\")\n",
    "print(f\"\\nüìä Linear R¬≤ improvement:\")\n",
    "print(f\"   Original: 0.5218\")\n",
    "print(f\"   Enhanced: {r2_linear_eng:.4f} (+{r2_linear_eng - 0.5218:.4f})\")\n",
    "print(f\"\\nüéØ Expected Neural Net R¬≤: 0.92-0.97\")\n",
    "print(f\"\\nüìã Next: Run Cell 9 to train with enhanced features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3fbc9",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 9: Train Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c554da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dense_model_path = 'models/saved/dense_pinn.pth'\n",
    "\n",
    "# Check if enhanced features available\n",
    "if 'X_train_tensor_eng' in globals():\n",
    "    print(\"üîß Using ENHANCED features from Cell 8\")\n",
    "    X_train_use = X_train_tensor_eng\n",
    "    y_train_use = y_train_tensor_eng\n",
    "    X_val_use = X_val_tensor_eng\n",
    "    y_val_use = y_val_tensor_eng\n",
    "    input_dim_use = input_dim_eng\n",
    "    output_dim_use = output_dim_eng\n",
    "else:\n",
    "    print(\"üìä Using ORIGINAL features (Cell 8 not run)\")\n",
    "    X_train_use = X_train_tensor\n",
    "    y_train_use = y_train_tensor\n",
    "    X_val_use = X_val_tensor\n",
    "    y_val_use = y_val_tensor\n",
    "    input_dim_use = input_dim\n",
    "    output_dim_use = output_dim\n",
    "\n",
    "print(\"\\nüèãÔ∏è Training from scratch (30-50 min)...\\n\")\n",
    "\n",
    "# Model\n",
    "dense_model = DensePINN(input_dim_use, [1024, 512, 512, 256, 128], output_dim_use, dropout=0.2).to(device)\n",
    "total_params = sum(p.numel() for p in dense_model.parameters())\n",
    "print(f\"Architecture: {input_dim_use} ‚Üí 1024 ‚Üí 512 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí {output_dim_use}\")\n",
    "print(f\"Parameters: {total_params:,}\")\n",
    "print(f\"Target: R¬≤ ‚â• 0.95\\n\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(dense_model.parameters(), lr=0.002, weight_decay=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_use, y_train_use)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):\n",
    "    dense_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = dense_model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(dense_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Evaluate every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        dense_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = dense_model(X_val_use)\n",
    "            val_loss = loss_fn(val_pred, y_val_use)\n",
    "            val_r2 = r2_score(y_val_use[:, 0].cpu().numpy(), val_pred[:, 0].cpu().numpy())\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        error_pct = (1 - val_r2) * 100\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d}: Loss={val_loss:.6f}, R¬≤={val_r2:.4f}, Error={error_pct:.2f}%, LR={current_lr:.6f}\")\n",
    "        \n",
    "        if val_r2 > best_r2:\n",
    "            best_r2 = val_r2\n",
    "            best_state = dense_model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"   ‚≠ê New best R¬≤! (Error: {(1-best_r2)*100:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if val_r2 >= 0.98:\n",
    "            print(f\"\\nüéâ EXCELLENT! R¬≤ ‚â• 0.98 achieved!\")\n",
    "            break\n",
    "        \n",
    "        if val_r2 >= 0.95 and epoch >= 100:\n",
    "            print(f\"\\n‚úÖ Target R¬≤ ‚â• 0.95 achieved!\")\n",
    "            break\n",
    "        \n",
    "        if patience_counter >= 40:\n",
    "            print(f\"\\n‚ö†Ô∏è Early stopping\")\n",
    "            break\n",
    "\n",
    "if best_state:\n",
    "    dense_model.load_state_dict(best_state)\n",
    "\n",
    "# Final evaluation\n",
    "dense_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred = dense_model(X_val_use)\n",
    "    final_r2 = r2_score(y_val_use[:, 0].cpu().numpy(), val_pred[:, 0].cpu().numpy())\n",
    "\n",
    "# Save\n",
    "os.makedirs(os.path.dirname(dense_model_path), exist_ok=True)\n",
    "torch.save(dense_model, dense_model_path)\n",
    "\n",
    "try:\n",
    "    drive_path = '/content/drive/MyDrive/SPINN_BACKUP/models/saved/dense_pinn.pth'\n",
    "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
    "    shutil.copy(dense_model_path, drive_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ TRAINING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best R¬≤: {best_r2:.4f}\")\n",
    "print(f\"Final R¬≤: {final_r2:.4f}\")\n",
    "print(f\"Parameters: {total_params:,}\")\n",
    "print(f\"üíæ Saved: {dense_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57c6ff",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 10: Structured Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "TARGET_SPARSITY = 0.80\n",
    "N_PRUNE_ROUNDS = 4\n",
    "EPOCHS_PER_ROUND = 40\n",
    "MIN_R2_THRESHOLD = 0.93\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"STRUCTURED PRUNING - Target: {TARGET_SPARSITY*100:.0f}% reduction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dense_params = sum(p.numel() for p in dense_model.parameters())\n",
    "keep_ratio = (1 - TARGET_SPARSITY) ** (1 / N_PRUNE_ROUNDS)\n",
    "\n",
    "# Determine which tensors to use\n",
    "if 'X_train_tensor_eng' in globals():\n",
    "    X_train_prune = X_train_tensor_eng\n",
    "    y_train_prune = y_train_tensor_eng\n",
    "    X_val_prune = X_val_tensor_eng\n",
    "    y_val_prune = y_val_tensor_eng\n",
    "    input_dim_prune = input_dim_eng\n",
    "    output_dim_prune = output_dim_eng\n",
    "else:\n",
    "    X_train_prune = X_train_tensor\n",
    "    y_train_prune = y_train_tensor\n",
    "    X_val_prune = X_val_tensor\n",
    "    y_val_prune = y_val_tensor\n",
    "    input_dim_prune = input_dim\n",
    "    output_dim_prune = output_dim\n",
    "\n",
    "spinn_model = DensePINN(input_dim_prune, [1024, 512, 512, 256, 128], output_dim_prune, dropout=0.15).to(device)\n",
    "spinn_model.load_state_dict(dense_model.state_dict())\n",
    "\n",
    "train_dataset = TensorDataset(X_train_prune, y_train_prune)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "for round_num in range(1, N_PRUNE_ROUNDS + 1):\n",
    "    print(f\"\\nüîÑ ROUND {round_num}/{N_PRUNE_ROUNDS}\")\n",
    "    \n",
    "    # Prune layers - get fresh list each round\n",
    "    # Handle both DensePINN and Sequential models\n",
    "    if hasattr(spinn_model, 'layers'):\n",
    "        all_layers = list(spinn_model.layers)\n",
    "    else:\n",
    "        all_layers = list(spinn_model)\n",
    "    \n",
    "    linear_layers = [m for m in all_layers if isinstance(m, nn.Linear)]\n",
    "    new_layers = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(all_layers):\n",
    "        layer = all_layers[i]\n",
    "        \n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Find which linear layer this is using identity comparison\n",
    "            linear_idx = None\n",
    "            for idx, lin_layer in enumerate(linear_layers):\n",
    "                if lin_layer is layer:\n",
    "                    linear_idx = idx\n",
    "                    break\n",
    "            \n",
    "            # If already replaced in this round, skip\n",
    "            if linear_idx is None:\n",
    "                new_layers.append(layer)\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Don't prune input or output layers\n",
    "            if linear_idx == 0 or linear_idx == len(linear_layers) - 1:\n",
    "                new_layers.append(layer)\n",
    "                i += 1\n",
    "            else:\n",
    "                # Find next linear layer\n",
    "                next_linear_idx = None\n",
    "                for j in range(i + 1, len(all_layers)):\n",
    "                    if isinstance(all_layers[j], nn.Linear):\n",
    "                        next_linear_idx = j\n",
    "                        break\n",
    "                \n",
    "                if next_linear_idx is not None:\n",
    "                    next_linear = all_layers[next_linear_idx]\n",
    "                    pruned_layer, pruned_next = prune_linear_layer(layer, next_linear, keep_ratio)\n",
    "                    \n",
    "                    new_layers.append(pruned_layer)\n",
    "                    \n",
    "                    # Handle intermediate layers (BatchNorm, ReLU, Dropout)\n",
    "                    for k in range(i + 1, next_linear_idx):\n",
    "                        intermediate = all_layers[k]\n",
    "                        if isinstance(intermediate, nn.BatchNorm1d):\n",
    "                            new_layers.append(nn.BatchNorm1d(pruned_layer.out_features))\n",
    "                        else:\n",
    "                            new_layers.append(intermediate)\n",
    "                    \n",
    "                    # Update the next linear layer in the list\n",
    "                    all_layers[next_linear_idx] = pruned_next\n",
    "                    i = next_linear_idx\n",
    "                else:\n",
    "                    new_layers.append(layer)\n",
    "                    i += 1\n",
    "        else:\n",
    "            # Non-linear layer - only add if not a BatchNorm that needs updating\n",
    "            if not any(isinstance(all_layers[j], nn.Linear) and j < i for j in range(max(0, i-3), i)):\n",
    "                new_layers.append(layer)\n",
    "            i += 1\n",
    "    \n",
    "    spinn_model = nn.Sequential(*new_layers).to(device)\n",
    "    \n",
    "    pruned_params = sum(p.numel() for p in spinn_model.parameters())\n",
    "    reduction = (1 - pruned_params / dense_params) * 100\n",
    "    print(f\"Parameters: {dense_params:,} ‚Üí {pruned_params:,} ({reduction:.1f}% reduction)\")\n",
    "    \n",
    "    # Fine-tune\n",
    "    optimizer = optim.AdamW(spinn_model.parameters(), lr=0.003, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PER_ROUND)\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(EPOCHS_PER_ROUND):\n",
    "        spinn_model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = spinn_model(X_batch)\n",
    "            loss = nn.MSELoss()(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(spinn_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == EPOCHS_PER_ROUND - 1:\n",
    "            spinn_model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = spinn_model(X_val_prune)\n",
    "                val_r2 = r2_score(y_val_prune[:, 0].cpu().numpy(), val_pred[:, 0].cpu().numpy())\n",
    "            \n",
    "            if val_r2 > best_r2:\n",
    "                best_r2 = val_r2\n",
    "                best_state = {k: v.cpu().clone() for k, v in spinn_model.state_dict().items()}\n",
    "    \n",
    "    if best_state:\n",
    "        spinn_model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    \n",
    "    print(f\"‚úÖ Best R¬≤: {best_r2:.4f}\")\n",
    "    \n",
    "    if best_r2 < MIN_R2_THRESHOLD:\n",
    "        print(f\"‚ö†Ô∏è R¬≤ < {MIN_R2_THRESHOLD}, stopping\")\n",
    "        break\n",
    "\n",
    "# Final evaluation\n",
    "spinn_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred = spinn_model(X_val_prune)\n",
    "    final_r2 = r2_score(y_val_prune[:, 0].cpu().numpy(), val_pred[:, 0].cpu().numpy())\n",
    "\n",
    "final_params = sum(p.numel() for p in spinn_model.parameters())\n",
    "final_reduction = (1 - final_params / dense_params) * 100\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ PRUNING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Dense:   {dense_params:,}\")\n",
    "print(f\"Pruned:  {final_params:,}\")\n",
    "print(f\"Reduction: {final_reduction:.1f}%\")\n",
    "print(f\"Final R¬≤: {final_r2:.4f}\")\n",
    "print(f\"Compression: {dense_params/final_params:.1f}x\")\n",
    "\n",
    "# Save\n",
    "spinn_path = f'models/saved/spinn_structured_{int(final_reduction)}pct.pth'\n",
    "os.makedirs(os.path.dirname(spinn_path), exist_ok=True)\n",
    "torch.save(spinn_model, spinn_path)\n",
    "\n",
    "try:\n",
    "    drive_path = f'/content/drive/MyDrive/SPINN_BACKUP/models/saved/spinn_structured_{int(final_reduction)}pct.pth'\n",
    "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
    "    shutil.copy(spinn_path, drive_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\nüíæ Saved: {spinn_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa7430",
   "metadata": {},
   "source": [
    "---\n",
    "# Cell 11: GPU Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf39ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GPU BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_trials = 200\n",
    "warmup = 50\n",
    "\n",
    "# Determine which validation tensor to use\n",
    "if 'X_val_tensor_eng' in globals():\n",
    "    X_val_bench = X_val_tensor_eng\n",
    "else:\n",
    "    X_val_bench = X_val_tensor\n",
    "\n",
    "# Dense model benchmark\n",
    "dense_model.eval()\n",
    "for _ in range(warmup):\n",
    "    with torch.no_grad():\n",
    "        _ = dense_model(X_val_bench)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "dense_times = []\n",
    "for _ in range(n_trials):\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = dense_model(X_val_bench)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        dense_times.append(start.elapsed_time(end))\n",
    "    else:\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = dense_model(X_val_bench)\n",
    "        end = time.perf_counter()\n",
    "        dense_times.append((end - start) * 1000)\n",
    "\n",
    "dense_median = np.median(dense_times)\n",
    "\n",
    "# SPINN model benchmark\n",
    "spinn_model.eval()\n",
    "for _ in range(warmup):\n",
    "    with torch.no_grad():\n",
    "        _ = spinn_model(X_val_bench)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "spinn_times = []\n",
    "for _ in range(n_trials):\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = spinn_model(X_val_bench)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        spinn_times.append(start.elapsed_time(end))\n",
    "    else:\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = spinn_model(X_val_bench)\n",
    "        end = time.perf_counter()\n",
    "        spinn_times.append((end - start) * 1000)\n",
    "\n",
    "spinn_median = np.median(spinn_times)\n",
    "speedup = dense_median / spinn_median\n",
    "\n",
    "print(f\"\\nDense:  {dense_median:.2f} ms\")\n",
    "print(f\"SPINN:  {spinn_median:.2f} ms\")\n",
    "print(f\"‚ö° SPEEDUP: {speedup:.2f}x\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BENCHMARK COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
